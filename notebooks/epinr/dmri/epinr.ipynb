{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c8cea842",
      "metadata": {},
      "source": [
        "# Susceptibility Distortion Correction for Diffusion Images\n",
        "\n",
        "Tyler Spears, Tom Fletcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe3b317a",
      "metadata": {},
      "outputs": [],
      "source": [
        "!cd \"${HOME}/Projects/mr-inr/notebooks/epinr/dmri/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4722c3d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "%autoreload 2\n",
        "# %matplotlib ipympl\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "# By default, allow memory allocation to expand to avoid OOM errors\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = os.environ.get(\n",
        "    \"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\"\n",
        ")\n",
        "# Force torch.load to use 'weights_only=False' by default, unless specified.\n",
        "# <https://pytorch.org/docs/main/notes/serialization.html#environment-variables>\n",
        "os.environ[\"TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\"] = os.environ.get(\n",
        "    \"TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\", \"1\"\n",
        ")\n",
        "# <https://github.com/pytorch/pytorch/issues/121197> and\n",
        "# <https://github.com/NVIDIA/NeMo-Curator/pull/34>\n",
        "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = os.environ.get(\n",
        "    \"TORCHINDUCTOR_COMPILE_THREADS\", \"1\"\n",
        ")\n",
        "ENABLE_CUDNN_BENCHMARK = bool(int(os.environ.get(\"ENABLE_CUDNN_BENCHMARK\", \"1\")))\n",
        "\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "# from dataclasses import dataclass\n",
        "import collections\n",
        "import dataclasses\n",
        "import tempfile\n",
        "import copy\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Any, Optional, Literal, Callable\n",
        "\n",
        "import torch.multiprocessing as multiprocessing  # noqa\n",
        "\n",
        "import einops\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from box import Box\n",
        "import yaml\n",
        "\n",
        "# visualization libraries\n",
        "# %matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import monai\n",
        "import monai.data\n",
        "import monai.transforms\n",
        "import monai.losses\n",
        "import kornia\n",
        "\n",
        "# Computation & ML libraries.\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import torch.func\n",
        "\n",
        "# The flag below controls whether to allow TF32 on matmul. Defaults to False\n",
        "# in PyTorch 1.12 and later. See\n",
        "# <https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices>\n",
        "# for details.\n",
        "##\n",
        "# Set this to True to force full precision matmuls, as registration can be sensitive to\n",
        "# precision.\n",
        "torch.backends.cuda.matmul.allow_tf32 = False\n",
        "torch.set_float32_matmul_precision(\"highest\")\n",
        "# The flag below controls whether to allow TF32 on cuDNN. Defaults to True.\n",
        "torch.backends.cudnn.allow_tf32 = False\n",
        "##\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.set_float32_matmul_precision(\"medium\")\n",
        "# # The flag below controls whether to allow TF32 on cuDNN. Defaults to True.\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "# ##\n",
        "# # Activate cudnn benchmarking to optimize convolution algorithm speed.\n",
        "# if torch.backends.cudnn.enabled and ENABLE_CUDNN_BENCHMARK:\n",
        "#     torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import mrinr\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from data_utils import (\n",
        "    PE_DIR_ALIASES,\n",
        "    DWISubjectData,\n",
        "    scale_vol,\n",
        "    blur_mask,\n",
        "    load_dwi_subject_data,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa4f78ed",
      "metadata": {},
      "source": [
        "## Function & Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2d4a178",
      "metadata": {},
      "outputs": [],
      "source": [
        "class WeightedNMIParzenLoss(torch.nn.modules.loss._Loss):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_bins: int = 32,\n",
        "        sigma_ratio: float = 0.5,\n",
        "        reduction: str = \"mean\",\n",
        "        eps: float = 1e-7,\n",
        "        norm_mi: bool = True,\n",
        "        norm_images: bool = True,\n",
        "    ):\n",
        "        super().__init__(reduction=reduction)\n",
        "        if num_bins <= 0:\n",
        "            raise ValueError(\"num_bins must > 0, got {num_bins}\")\n",
        "        self.spatial_dims = 3\n",
        "        self.num_bins = int(num_bins)\n",
        "        # shape (num_bins)\n",
        "        bin_centers = torch.linspace(0.0, 1.0, self.num_bins)\n",
        "        self.register_buffer(\"bin_centers\", bin_centers)\n",
        "        self.bin_centers: torch.Tensor\n",
        "\n",
        "        sigma = torch.mean(self.bin_centers[1:] - self.bin_centers[:-1]) * sigma_ratio\n",
        "        self.register_buffer(\"sigma\", sigma)\n",
        "        self.sigma: torch.Tensor\n",
        "        self.eps = eps\n",
        "        self.norm_mi = norm_mi\n",
        "        self.norm_images = norm_images\n",
        "\n",
        "    @staticmethod\n",
        "    def spatial_normalize(x: torch.Tensor, eps: float) -> torch.Tensor:\n",
        "        \"\"\"Min-max normalize x to [0, 1] along spatial dimensions.\"\"\"\n",
        "        x_min = einops.reduce(x, \"b c x y z -> b c 1 1 1\", \"min\")\n",
        "        x_max = einops.reduce(x, \"b c x y z -> b c 1 1 1\", \"max\")\n",
        "        x_normalized = (x - x_min) / (x_max - x_min + eps)\n",
        "        return x_normalized\n",
        "\n",
        "    def parzen_windowing_gaussian(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Parzen Gaussian weighting function to approximate histogram differentiably.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor of shape B x C x X x Y x Z.\n",
        "\n",
        "            *NOTE* Input volume must have intensities normalized to [0, 1]. Intensities\n",
        "            will be clamped to [0, 1] internally.\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Discrete probability distributions for each voxel, shape\n",
        "            (B*C) x (X*Y*Z) x num_bins. Distributions are not averaged over spatial\n",
        "            dimensions.\n",
        "        \"\"\"\n",
        "        y = torch.clamp(x, 0.0, 1.0)\n",
        "        # Move independent dims to the front, and merge spatial dims into a sampling\n",
        "        # dimension, plus a singleton dimension for broadcasting hist. bins.\n",
        "        y = einops.rearrange(y, \"b c ... -> (b c) (...) 1\")\n",
        "        w_parzen = (1 / (self.sigma * math.sqrt(2 * math.pi))) * torch.exp(\n",
        "            -0.5 * ((y - self.bin_centers[None, None, :]) / self.sigma) ** 2\n",
        "        )\n",
        "        # Normalize over bins.\n",
        "        p = w_parzen / torch.maximum(\n",
        "            torch.sum(w_parzen, dim=-1, keepdim=True),\n",
        "            w_parzen.new_tensor([self.eps]),\n",
        "        )\n",
        "        # Wait to average over sampling dimensions until estimating the joint histogram.\n",
        "        return p\n",
        "\n",
        "    def weighted_nmi(\n",
        "        self,\n",
        "        pred: mrinr.typing.ScalarVolume,\n",
        "        target: mrinr.typing.ScalarVolume,\n",
        "        weight_mask: Optional[mrinr.typing.ScalarVolume] = None,\n",
        "        norm_mi: bool = True,\n",
        "        normalize_images: bool = True,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Args:\n",
        "            pred: the shape should be B[NDHW].\n",
        "            target: the shape should be same as the pred shape.\n",
        "            weight_mask: the shape should be B[1DHW] or B[NDHW], optional.\n",
        "        Raises:\n",
        "            ValueError: When ``self.reduction`` is not one of [\"mean\", \"sum\", \"none\"].\n",
        "        \"\"\"\n",
        "        if target.shape != pred.shape:\n",
        "            raise ValueError(\n",
        "                f\"ground truth has differing shape ({target.shape}) from pred ({pred.shape})\"\n",
        "            )\n",
        "\n",
        "        if normalize_images:\n",
        "            # Normalize pred and target to [0, 1] along spatial dims, keeping batch and\n",
        "            # channel dims independent.\n",
        "            x = self.spatial_normalize(pred, eps=self.eps)\n",
        "            y = self.spatial_normalize(target, eps=self.eps)\n",
        "        else:\n",
        "            x = pred\n",
        "            y = target\n",
        "\n",
        "        # Parzen windowing, without averaging over samples. Outputs are\n",
        "        # shape (B*C)(D*H*W)(num_bins).\n",
        "        p_pred = self.parzen_windowing_gaussian(x)\n",
        "        p_target = self.parzen_windowing_gaussian(y)\n",
        "\n",
        "        # Estimate joint histogram P_pred,target weighted by the weight mask if\n",
        "        # provided.\n",
        "        if weight_mask is not None:\n",
        "            w = einops.rearrange(\n",
        "                weight_mask.expand_as(pred), \"b c x y z -> (b c) (x y z) 1 1\"\n",
        "            )\n",
        "            # Normalize the weight mask to be in the range [0, 1].\n",
        "            w = w / torch.maximum(\n",
        "                w.max(dim=1, keepdim=True).values, w.new_tensor([self.eps])\n",
        "            )\n",
        "        else:\n",
        "            w = 1.0\n",
        "        p_joint = (\n",
        "            w * einops.einsum(p_pred, p_target, \"bc xyz i, bc xyz j -> bc xyz i j\")\n",
        "        ).sum(1)\n",
        "        # Normalize joint histogram.\n",
        "        p_joint = p_joint / torch.maximum(\n",
        "            p_joint.sum(dim=(-2, -1), keepdim=True),\n",
        "            p_joint.new_tensor([self.eps]),\n",
        "        )\n",
        "        # Estimate marginal histograms.\n",
        "        p_pred_marginal = p_joint.sum(dim=-1)\n",
        "        p_target_marginal = p_joint.sum(dim=-2)\n",
        "\n",
        "        # Compute entropies.\n",
        "        H_pred = -torch.sum(\n",
        "            p_pred_marginal\n",
        "            * torch.log(\n",
        "                torch.maximum(p_pred_marginal, p_pred_marginal.new_tensor([self.eps]))\n",
        "            ),\n",
        "            dim=-1,\n",
        "        )\n",
        "        H_target = -torch.sum(\n",
        "            p_target_marginal\n",
        "            * torch.log(\n",
        "                torch.maximum(\n",
        "                    p_target_marginal, p_target_marginal.new_tensor([self.eps])\n",
        "                )\n",
        "            ),\n",
        "            dim=-1,\n",
        "        )\n",
        "        H_joint = -torch.sum(\n",
        "            p_joint * torch.log(torch.maximum(p_joint, p_joint.new_tensor([self.eps]))),\n",
        "            dim=(-2, -1),\n",
        "        )\n",
        "\n",
        "        # NMI or plain MI.\n",
        "        if norm_mi:\n",
        "            mi = (H_pred + H_target) / torch.maximum(\n",
        "                H_joint, H_joint.new_tensor([self.eps])\n",
        "            )\n",
        "        else:\n",
        "            mi = H_pred + H_target - H_joint\n",
        "\n",
        "        if self.reduction == \"sum\":\n",
        "            # sum over the batch and channel ndims\n",
        "            r = torch.sum(mi)\n",
        "        elif self.reduction == \"none\":\n",
        "            # No reduction of independent dims.\n",
        "            r = einops.rearrange(mi, \"(b c) -> b c\", b=pred.shape[0], c=pred.shape[1])\n",
        "        elif self.reduction == \"mean\":\n",
        "            # average over the batch and channel ndims\n",
        "            r = torch.mean(mi)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Unsupported reduction: {self.reduction}, \"\n",
        "                'available options are [\"mean\", \"sum\", \"none\"].'\n",
        "            )\n",
        "        return r\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pred: mrinr.typing.ScalarVolume,\n",
        "        target: mrinr.typing.ScalarVolume,\n",
        "        weight_mask: Optional[mrinr.typing.ScalarVolume] = None,\n",
        "    ) -> torch.Tensor:\n",
        "        # Loss is negative NMI.\n",
        "        return -self.weighted_nmi(\n",
        "            pred,\n",
        "            target,\n",
        "            weight_mask=weight_mask,\n",
        "            norm_mi=self.norm_mi,\n",
        "            normalize_images=self.norm_images,\n",
        "        )\n",
        "\n",
        "\n",
        "class WeightedNCCLoss(torch.nn.modules.loss._Loss):\n",
        "    def __init__(self, eps: float = 1e-8):\n",
        "        \"\"\"Compute weighted normalized cross-correlation (NCC) between two 3D images.\n",
        "        Args:\n",
        "            img1, img2: tensors of shape (H, W, D)\n",
        "            weight: optional tensor of same shape (H, W, D), spatial weights in [0, 1].\n",
        "                    If None, uniform weights are used.\n",
        "            eps: small constant for numerical stability.\n",
        "\n",
        "        Returns:\n",
        "            scalar tensor: weighted NCC value\n",
        "        \"\"\"\n",
        "        super().__init__(reduction=\"mean\")\n",
        "        self.eps = eps\n",
        "\n",
        "    def weighted_ncc(\n",
        "        self,\n",
        "        pred: mrinr.typing.ScalarVolume,\n",
        "        target: mrinr.typing.ScalarVolume,\n",
        "        weight_mask: Optional[mrinr.typing.ScalarVolume] = None,\n",
        "    ):\n",
        "        # assert img1.shape == img2.shape, \"Input images must have the same shape\"\n",
        "\n",
        "        # assert x1.shape == x2.shape, \"Inputs are not of similar shape\"\n",
        "        # cc = ((x1 - x1.mean()) * (x2 - x2.mean())).mean()\n",
        "        # stablestd = self._StableStd.apply\n",
        "        # std = stablestd(x1) * stablestd(x2)\n",
        "        # ncc = cc / (std + e)\n",
        "        # return ncc\n",
        "\n",
        "        x = einops.rearrange(pred, \"b c x y z -> b c (x y z)\")\n",
        "        y = einops.rearrange(target, \"b c x y z -> b c (x y z)\")\n",
        "        if weight_mask is not None:\n",
        "            w = einops.rearrange(\n",
        "                weight_mask.expand_as(pred), \"b c x y z -> b c (x y z)\"\n",
        "            )\n",
        "        else:\n",
        "            w = torch.ones_like(x)\n",
        "\n",
        "        if weight is None:\n",
        "            w = torch.ones_like(i1)\n",
        "        else:\n",
        "            w = weight.flatten()\n",
        "            w = w / (w.sum() + self.eps)  # normalize weights\n",
        "\n",
        "        # Weighted means\n",
        "        mu1 = torch.sum(w * i1)\n",
        "        mu2 = torch.sum(w * i2)\n",
        "\n",
        "        # Weighted covariance and variances\n",
        "        v1 = i1 - mu1\n",
        "        v2 = i2 - mu2\n",
        "\n",
        "        cov12 = torch.sum(w * v1 * v2)\n",
        "        var1 = torch.sum(w * v1 * v1)\n",
        "        var2 = torch.sum(w * v2 * v2)\n",
        "\n",
        "        # Normalized cross-correlation\n",
        "        ncc = cov12 / (torch.sqrt(var1 * var2) + self.eps)\n",
        "        return ncc\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pred: mrinr.typing.ScalarVolume,\n",
        "        target: mrinr.typing.ScalarVolume,\n",
        "        weight_mask: Optional[mrinr.typing.ScalarVolume] = None,\n",
        "    ) -> torch.Tensor:\n",
        "        # Loss is negative NCC.\n",
        "        return -self.weighted_ncc(pred, target, weight_mask=weight_mask)\n",
        "\n",
        "\n",
        "class CoordSampledWeightedMINDLoss(torch.nn.modules.loss._Loss):\n",
        "    def __init__(\n",
        "        self,\n",
        "        patch_size: int = 3,\n",
        "        patch_comparison_distance: int = 2,\n",
        "        reduction: str = \"mean\",\n",
        "        eps: float = 1e-7,\n",
        "        **patch_sampling_kwargs,\n",
        "    ):\n",
        "        super().__init__(reduction=reduction)\n",
        "        self.spatial_dims = 3\n",
        "        self.eps = eps\n",
        "        self._patch_sampling_kwargs = patch_sampling_kwargs\n",
        "        if not self._patch_sampling_kwargs:\n",
        "            self._patch_sampling_kwargs = dict(\n",
        "                mode_or_interpolation=\"linear\",\n",
        "                padding_mode_or_bound=\"zeros\",\n",
        "                interp_lib=\"torch\",\n",
        "            )\n",
        "\n",
        "    @staticmethod\n",
        "    def spatial_normalize(x: torch.Tensor, eps: float) -> torch.Tensor:\n",
        "        \"\"\"Min-max normalize x to [0, 1] along spatial dimensions.\"\"\"\n",
        "        x_min = einops.reduce(x, \"b c x y z -> b c 1 1 1\", \"min\")\n",
        "        x_max = einops.reduce(x, \"b c x y z -> b c 1 1 1\", \"max\")\n",
        "        x_normalized = (x - x_min) / (x_max - x_min + eps)\n",
        "        return x_normalized\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        moving_coords: mrinr.typing.CoordGrid3D,\n",
        "        fixed_coords: mrinr.typing.CoordGrid3D,\n",
        "        moving_volume: mrinr.typing.SingleScalarVolume,\n",
        "        fixed_volume: mrinr.typing.SingleScalarVolume,\n",
        "        moving_affine: mrinr.typing.SingleHomogeneousAffine3D,\n",
        "        fixed_affine: mrinr.typing.SingleHomogeneousAffine3D,\n",
        "        weight_mask: Optional[mrinr.typing.ScalarVolume] = None,\n",
        "    ) -> torch.Tensor:\n",
        "        if moving_volume.ndim == 5:\n",
        "            moving_volume = moving_volume.squeeze(0)\n",
        "        if fixed_volume.ndim == 5:\n",
        "            fixed_volume = fixed_volume.squeeze(0)\n",
        "        if moving_affine.ndim == 3:\n",
        "            moving_affine = moving_affine.squeeze(0)\n",
        "        if fixed_affine.ndim == 3:\n",
        "            fixed_affine = fixed_affine.squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed1846db",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss functions\n",
        "class NCC(torch.nn.modules.loss._Loss):\n",
        "    # Normalized Cross Correlation\n",
        "    # Taken from <https://github.com/MIAGroupUT/IDIR/blob/main/objectives/ncc.py>,\n",
        "    # which itself was taken from <https://github.com/BDdeVos/TorchIR>\n",
        "    class _StableStd(torch.autograd.Function):\n",
        "        @staticmethod\n",
        "        def forward(ctx, tensor):\n",
        "            assert tensor.numel() > 1\n",
        "            ctx.tensor = tensor.detach()\n",
        "            res = torch.std(tensor).detach()\n",
        "            ctx.result = res.detach()\n",
        "            return res\n",
        "\n",
        "        @staticmethod\n",
        "        def backward(ctx, grad_output):\n",
        "            tensor = ctx.tensor.detach()\n",
        "            result = ctx.result.detach()\n",
        "            e = 1e-6\n",
        "            assert tensor.numel() > 1\n",
        "            return (\n",
        "                (2.0 / (tensor.numel() - 1.0))\n",
        "                * (grad_output.detach() / (result.detach() * 2 + e))\n",
        "                * (tensor.detach() - tensor.mean().detach())\n",
        "            )\n",
        "\n",
        "    def __init__(self, use_mask: bool = False):\n",
        "        super().__init__()\n",
        "        self.forward = self.metric\n",
        "\n",
        "    def ncc(self, x1, x2, e=1e-10):\n",
        "        assert x1.shape == x2.shape, \"Inputs are not of similar shape\"\n",
        "        cc = ((x1 - x1.mean()) * (x2 - x2.mean())).mean()\n",
        "        stablestd = self._StableStd.apply\n",
        "        std = stablestd(x1) * stablestd(x2)\n",
        "        ncc = cc / (std + e)\n",
        "        return ncc\n",
        "\n",
        "    def metric(self, fixed: torch.Tensor, warped: torch.Tensor) -> torch.Tensor:\n",
        "        return -self.ncc(fixed, warped)\n",
        "\n",
        "\n",
        "class DoGLaplacian(torch.nn.Module):\n",
        "    \"\"\"Difference of Gaussians Laplacian filter.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sigma_low: float | tuple,\n",
        "        truncate: float | tuple,\n",
        "        border_type: str = \"replicate\",\n",
        "        normalized: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.border_type = border_type\n",
        "        self.normalized = normalized\n",
        "\n",
        "        if isinstance(sigma_low, (float, int)):\n",
        "            sigma_low = [sigma_low, sigma_low, sigma_low]\n",
        "        sigma_low = np.asarray(sigma_low, dtype=np.float32)\n",
        "        # Scale by 1.6 to get the high sigma that approximates the Laplacian.\n",
        "        sigma_high = sigma_low * 1.6\n",
        "        if isinstance(truncate, (float, int)):\n",
        "            truncate = [truncate, truncate, truncate]\n",
        "        truncate = np.asarray(truncate, dtype=np.float32)\n",
        "        kernel_size_low = 2 * np.ceil(truncate * sigma_low).astype(int) + 1\n",
        "        kernel_size_high = 2 * np.ceil(truncate * sigma_high).astype(int) + 1\n",
        "\n",
        "        self.sigma_low = tuple(sigma_low.tolist())\n",
        "        self.sigma_high = tuple(sigma_high.tolist())\n",
        "        self.kernel_size_low = tuple(kernel_size_low.tolist())\n",
        "        self.kernel_size_high = tuple(kernel_size_high.tolist())\n",
        "\n",
        "        gaussian_kernel_low = kornia.filters.get_gaussian_kernel3d(\n",
        "            kernel_size=kernel_size_low, sigma=self.sigma_low, dtype=torch.float32\n",
        "        )\n",
        "        self.register_buffer(\"gaussian_kernel_low\", gaussian_kernel_low)\n",
        "        self.gaussian_kernel_low: torch.Tensor\n",
        "        gaussian_kernel_high = kornia.filters.get_gaussian_kernel3d(\n",
        "            kernel_size=kernel_size_high, sigma=self.sigma_high, dtype=torch.float32\n",
        "        )\n",
        "        self.register_buffer(\"gaussian_kernel_high\", gaussian_kernel_high)\n",
        "        self.gaussian_kernel_high: torch.Tensor\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply DoG Laplacian filter to input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (B, C, X, Y, Z).\n",
        "\n",
        "        Returns:\n",
        "            Tensor of same shape as input, after applying DoG Laplacian filter.\n",
        "        \"\"\"\n",
        "        low_pass = kornia.filters.filter3d(\n",
        "            x,\n",
        "            self.gaussian_kernel_low,\n",
        "            border_type=self.border_type,\n",
        "            normalized=self.normalized,\n",
        "        )\n",
        "        high_pass = kornia.filters.filter3d(\n",
        "            x,\n",
        "            self.gaussian_kernel_high,\n",
        "            border_type=self.border_type,\n",
        "            normalized=self.normalized,\n",
        "        )\n",
        "        dog_laplacian = low_pass - high_pass\n",
        "        return dog_laplacian\n",
        "\n",
        "\n",
        "def displace_mag_regularization(\n",
        "    displacement_field: mrinr.typing.CoordGrid3D,\n",
        "    norm_ord: int = 2,\n",
        ") -> torch.Tensor:\n",
        "    return torch.linalg.norm(displacement_field, ord=norm_ord, dim=-1).mean()\n",
        "\n",
        "\n",
        "def jacobian_matrix_batched(\n",
        "    pred_displacement_net: torch.nn.Module,\n",
        "    x_coords: mrinr.typing.ScalarVolume,\n",
        "    grid_fov: torch.Tensor,\n",
        "    grid_min_coord: torch.Tensor,\n",
        "    jac_of_warp_type: str = \"deformation\",\n",
        "    compile_jacobian: bool = False,\n",
        "):\n",
        "    jac_of_warp_type = jac_of_warp_type.lower().strip()\n",
        "    deformation_grad = \"deform\" in jac_of_warp_type\n",
        "    displacement_grad = \"displac\" in jac_of_warp_type\n",
        "    assert (\n",
        "        deformation_grad ^ displacement_grad\n",
        "    ), \"Either deformation_grad or displacement_grad must be True.\"\n",
        "    is_training = pred_displacement_net.training\n",
        "    # Set network to eval mode to disable batchnorms, but keep gradients enabled.\n",
        "    pred_displacement_net.eval()\n",
        "\n",
        "    # Reshape everything to be of shape B x 3, to make the vmap easier.\n",
        "    orig_x_shape = tuple(x_coords.shape)\n",
        "    x_coords = einops.rearrange(\n",
        "        x_coords,\n",
        "        \"b x y z coord -> (b x y z) coord\",\n",
        "        coord=3,\n",
        "    )\n",
        "    grid_fov = einops.rearrange(\n",
        "        grid_fov,\n",
        "        \"b x y z coord -> (b x y z) coord\",\n",
        "        coord=3,\n",
        "    )\n",
        "    grid_min_coord = einops.rearrange(\n",
        "        grid_min_coord,\n",
        "        \"b x y z coord -> (b x y z) coord\",\n",
        "        coord=3,\n",
        "    )\n",
        "    # Create function for the Jacobian of the *deformation* field using\n",
        "    # reverse-mode auto-diff.\n",
        "    if deformation_grad:\n",
        "        jac_fn = torch.func.jacrev(\n",
        "            lambda x, f, m: pred_displacement_net(x, f, m) + x, argnums=0\n",
        "        )\n",
        "    else:\n",
        "        jac_fn = torch.func.jacrev(\n",
        "            lambda x, f, m: pred_displacement_net(x, f, m), argnums=0\n",
        "        )\n",
        "    # Vectorize the Jacobian function over the batch size.\n",
        "    # Ensure all Tensors have the same batch size.\n",
        "    grid_fov = grid_fov.expand_as(x_coords)\n",
        "    grid_min_coord = grid_min_coord.expand_as(x_coords)\n",
        "    v_jac_fn = torch.vmap(jac_fn, in_dims=(0, 0, 0), out_dims=0, randomness=\"error\")\n",
        "    if compile_jacobian:\n",
        "        v_jac_fn = torch.compile(v_jac_fn)\n",
        "\n",
        "    # Compute the Jacobian.\n",
        "    J = v_jac_fn(x_coords, grid_fov, grid_min_coord)\n",
        "    # Reshape the batch dims to match the input shape.\n",
        "    J = einops.rearrange(\n",
        "        J,\n",
        "        \"(b x y z) n_coord m_coord -> b x y z n_coord m_coord\",\n",
        "        b=orig_x_shape[0],\n",
        "        x=orig_x_shape[1],\n",
        "        y=orig_x_shape[2],\n",
        "        z=orig_x_shape[3],\n",
        "    )\n",
        "\n",
        "    pred_displacement_net.train(is_training)\n",
        "    return J\n",
        "\n",
        "\n",
        "def bending_energy_regularization(\n",
        "    pred_displacement_net: torch.nn.Module,\n",
        "    x_coords: mrinr.typing.ScalarVolume,\n",
        "    grid_fov: torch.Tensor,\n",
        "    grid_min_coord: torch.Tensor,\n",
        "    pe_dir: str,\n",
        "    hessian_of_warp_type: str = \"deformation\",\n",
        "    compile_hessian: bool = False,\n",
        ") -> torch.Tensor:\n",
        "    hess_of_warp_type = hessian_of_warp_type.lower().strip()\n",
        "    deformation_grad = \"deform\" in hess_of_warp_type\n",
        "    displacement_grad = \"displac\" in hess_of_warp_type\n",
        "    assert (\n",
        "        deformation_grad ^ displacement_grad\n",
        "    ), \"Either deformation_grad or displacement_grad must be True.\"\n",
        "    is_training = pred_displacement_net.training\n",
        "    # Set network to eval mode to disable batchnorms, but keep gradients enabled.\n",
        "    pred_displacement_net.eval()\n",
        "\n",
        "    # Reshape everything to be of shape B x 3, to make the vmap easier.\n",
        "    orig_x_shape = tuple(x_coords.shape)\n",
        "    x_coords = einops.rearrange(\n",
        "        x_coords,\n",
        "        \"b x y z coord -> (b x y z) coord\",\n",
        "        coord=3,\n",
        "    )\n",
        "    orig_grid_fov = grid_fov\n",
        "    grid_fov = einops.rearrange(\n",
        "        orig_grid_fov,\n",
        "        \"b x y z coord -> (b x y z) coord\",\n",
        "        coord=3,\n",
        "    )\n",
        "    grid_min_coord = einops.rearrange(\n",
        "        grid_min_coord,\n",
        "        \"b x y z coord -> (b x y z) coord\",\n",
        "        coord=3,\n",
        "    )\n",
        "    # Create function for the Hessian of the *deformation* field using\n",
        "    # reverse-mode auto-diff.\n",
        "    if deformation_grad:\n",
        "        hessian_fn = torch.func.hessian(\n",
        "            lambda x, f, m: pred_displacement_net(x, f, m) + x, argnums=0\n",
        "        )\n",
        "    else:\n",
        "        hessian_fn = torch.func.hessian(\n",
        "            lambda x, f, m: pred_displacement_net(x, f, m), argnums=0\n",
        "        )\n",
        "\n",
        "    pe_dir = PE_DIR_ALIASES[pe_dir]\n",
        "    # The Hessian should only be non-zero along the PE direction.\n",
        "    if pe_dir in {\"ap\", \"pa\"}:\n",
        "        hessian_fn_pe = lambda x, f, m: hessian_fn(x, f, m)[\n",
        "            ..., 1\n",
        "        ]  # Y coordinate of the output vector.\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported PE direction: {pe_dir}\")\n",
        "\n",
        "    # Vectorize the Hessian function over the batch size.\n",
        "    # Ensure all Tensors have the same batch size.\n",
        "    grid_fov = grid_fov.expand_as(x_coords)\n",
        "    grid_min_coord = grid_min_coord.expand_as(x_coords)\n",
        "    v_hessian_fn_pe = torch.vmap(\n",
        "        hessian_fn_pe, in_dims=(0, 0, 0), out_dims=0, randomness=\"error\"\n",
        "    )\n",
        "    if compile_hessian:\n",
        "        v_hessian_fn_pe = torch.compile(v_hessian_fn_pe)\n",
        "    # Compute the full 3D, batched Hessian.\n",
        "    H_pe = v_hessian_fn_pe(x_coords, grid_fov, grid_min_coord)\n",
        "    # Reshape the dims to match the input shape.\n",
        "    # Shape of Hessian tensor without batch dims:\n",
        "    # out_dims_m x in_dims_n x in_dims_n\n",
        "    # which is just a batch of Hessian matrices such that for every output dimension k,\n",
        "    # there is a Hessian matrix H_k of all input dimension combinations:\n",
        "    # H_k_ij = d^2 f_k / (d x_i d x_j)\n",
        "    H_pe = einops.rearrange(\n",
        "        H_pe,\n",
        "        \"(b x y z) n_coord_1 n_coord_2 -> b x y z n_coord_1 n_coord_2\",\n",
        "        b=orig_x_shape[0],\n",
        "        x=orig_x_shape[1],\n",
        "        y=orig_x_shape[2],\n",
        "        z=orig_x_shape[3],\n",
        "    )\n",
        "\n",
        "    # Normalizing domain volume/size.\n",
        "    # V = orig_grid_fov.reshape(orig_x_shape[0], 3).prod(dim=-1)\n",
        "    # # Compute bending energy.\n",
        "    # # Sum over all spatial locations and coordinate directions, normalizing by batch-wise\n",
        "    # # FOV/volume.\n",
        "    # l_bending = (\n",
        "    #     einops.reduce(\n",
        "    #         H_pe**2,\n",
        "    #         \"b x y z n_coord_1 n_coord_2 -> b\",\n",
        "    #         \"sum\",\n",
        "    #         b=orig_x_shape[0],\n",
        "    #         x=orig_x_shape[1],\n",
        "    #         y=orig_x_shape[2],\n",
        "    #         z=orig_x_shape[3],\n",
        "    #         n_coord_1=3,\n",
        "    #         n_coord_2=3,\n",
        "    #     )\n",
        "    #     / V\n",
        "    # )\n",
        "    #\n",
        "    # Compute bending energy.\n",
        "    # Sum over all spatial locations and coordinate directions, normalizing by batch-wise\n",
        "    # FOV/volume.\n",
        "    l_bending = einops.reduce(\n",
        "        H_pe**2,\n",
        "        \"b x y z n_coord_1 n_coord_2 -> b x y z\",\n",
        "        \"sum\",\n",
        "        b=orig_x_shape[0],\n",
        "        x=orig_x_shape[1],\n",
        "        y=orig_x_shape[2],\n",
        "        z=orig_x_shape[3],\n",
        "        n_coord_1=3,\n",
        "        n_coord_2=3,\n",
        "    ).mean()\n",
        "    #\n",
        "\n",
        "    pred_displacement_net.train(is_training)\n",
        "    return l_bending\n",
        "\n",
        "\n",
        "def smooth_displace_regularization(J_displacement: torch.Tensor) -> torch.Tensor:\n",
        "    spatial_dims = 3\n",
        "    J = J_displacement\n",
        "    # Last 2 dimensions of J should be the coordinate dimensions.\n",
        "    assert J.shape[-1] == J.shape[-2] == spatial_dims\n",
        "    J = J.view(-1, spatial_dims, spatial_dims)\n",
        "    # Correct implementation?\n",
        "    l_smooth = torch.linalg.matrix_norm(J, ord=\"fro\") ** 2\n",
        "    l_smooth = (l_smooth / spatial_dims).mean()\n",
        "    return l_smooth\n",
        "\n",
        "\n",
        "def jac_deform_regularization(J_deform: torch.Tensor) -> torch.Tensor:\n",
        "    spatial_dims = 3\n",
        "    # Compute the Jacobian determinant for regularization.\n",
        "    # Last 2 dimensions of J should be the coordinate dimensions.\n",
        "    J = J_deform\n",
        "    assert J.shape[-1] == J.shape[-2] == spatial_dims\n",
        "    J = J.view(-1, spatial_dims, spatial_dims)\n",
        "    batch_size = J.shape[0]\n",
        "    l = torch.abs(1 - torch.linalg.det(J)).sum()\n",
        "    l /= batch_size\n",
        "    return l\n",
        "\n",
        "\n",
        "def jac_displace_fro_regularization(J_displace: torch.Tensor) -> torch.Tensor:\n",
        "    spatial_dims = 3\n",
        "    # Compute the Jacobian determinant for regularization.\n",
        "    # Last 2 dimensions of J should be the coordinate dimensions.\n",
        "    J = J_displace\n",
        "    assert J.shape[-1] == J.shape[-2] == spatial_dims\n",
        "    J = J.view(-1, spatial_dims, spatial_dims)\n",
        "    l = torch.linalg.matrix_norm(J, ord=\"fro\", dim=(-2, -1)) ** 2\n",
        "    l = l.mean()\n",
        "    return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93613193",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Siren(torch.nn.Module):\n",
        "    \"\"\"SIREN implicit neural representation.\n",
        "\n",
        "    From:\n",
        "    V. Sitzmann, J. N. P. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein,\n",
        "        \u201cImplicit Neural Representations with Periodic Activation Functions.\u201d\n",
        "        arXiv, Jun. 17, 2020. doi: 10.48550/arXiv.2006.09661.\n",
        "\n",
        "    Implementation adapated from <https://github.com/lucidrains/siren-pytorch>\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        hidden_features,\n",
        "        out_features,\n",
        "        num_layers,\n",
        "        omega=1.0,\n",
        "        omega_0=30.0,\n",
        "        c: float = 6.0,\n",
        "        bias=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_features = hidden_features\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.c = c\n",
        "\n",
        "        layers = list()\n",
        "        for idx in range(num_layers):\n",
        "            is_first = idx == 0\n",
        "            omega_i = omega_0 if is_first else omega\n",
        "            in_features_i = self.in_features if is_first else self.hidden_features\n",
        "\n",
        "            layer = SirenBlock(\n",
        "                omega=omega_i,\n",
        "                is_first_layer=is_first,\n",
        "                c=self.c,\n",
        "                use_activation=True,\n",
        "                in_features=in_features_i,\n",
        "                out_features=self.hidden_features,\n",
        "                bias=bias,\n",
        "            )\n",
        "\n",
        "            layers.append(layer)\n",
        "\n",
        "        # Final layer without activation or bias. As a displacement field, there should\n",
        "        # be no global translation component, so the bias is disabled.\n",
        "        final_layer = SirenBlock(\n",
        "            omega=omega,\n",
        "            c=self.c,\n",
        "            is_first_layer=False,\n",
        "            use_activation=False,\n",
        "            in_features=self.hidden_features,\n",
        "            out_features=self.out_features,\n",
        "            bias=False,\n",
        "        )\n",
        "        layers.append(final_layer)\n",
        "\n",
        "        self.linear_layers = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, return_intermediates: bool = False):\n",
        "        if not return_intermediates:\n",
        "            y = self.linear_layers(x)\n",
        "        else:\n",
        "            y_i = x\n",
        "            intermediates = list()\n",
        "            for lin in self.linear_layers:\n",
        "                y_i = lin(y_i)\n",
        "                intermediates.append(y_i)\n",
        "            y = intermediates[-1], tuple(intermediates[:-1])\n",
        "        return y\n",
        "\n",
        "\n",
        "class SirenBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        omega: float,\n",
        "        is_first_layer: bool,\n",
        "        c: float = 6,\n",
        "        use_activation: bool = True,\n",
        "        activate_fn_kwargs: dict = dict(),\n",
        "        **linear_kwargs,\n",
        "    ):\n",
        "        \"\"\"SIREN implicit neural representation linear and activation block.\n",
        "\n",
        "        From:\n",
        "        V. Sitzmann, J. N. P. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein,\n",
        "            \u201cImplicit Neural Representations with Periodic Activation Functions.\u201d\n",
        "            arXiv, Jun. 17, 2020. doi: 10.48550/arXiv.2006.09661.\n",
        "\n",
        "        Implementation adapated from <https://github.com/lucidrains/siren-pytorch>\n",
        "        and <https://github.com/vsitzmann/siren/blob/master/explore_siren.ipynb>.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        omega : float\n",
        "            Sine scale parameter; determines sine weight and linear weight init.\n",
        "\n",
        "            In the original paper, omega = 30 was chosen for many tests.\n",
        "        is_first_layer : bool\n",
        "            Indicator for being the first layer in the INR MLP.\n",
        "        c : float, optional\n",
        "            Constant to drive initial weight distribution, by default 6.\n",
        "\n",
        "            This should probably not be changed, but it probably shouldn't be hardcoded.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = torch.nn.Linear(**linear_kwargs)\n",
        "        if use_activation:\n",
        "            self.activate_fn = mrinr.nn.Sine(omega, **activate_fn_kwargs)\n",
        "        else:\n",
        "            self.activate_fn = None\n",
        "\n",
        "        self._init_siren_linear_(\n",
        "            self.linear, omega=omega, is_first_layer=is_first_layer, c=c\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.linear(x)\n",
        "        if self.activate_fn is not None:\n",
        "            y = self.activate_fn(y)\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_siren_linear_(\n",
        "        l: torch.nn.Linear,\n",
        "        omega: float,\n",
        "        is_first_layer: bool,\n",
        "        c: float,\n",
        "    ):\n",
        "        n = l.in_features\n",
        "        if is_first_layer:\n",
        "            omega_bound = 1 / n\n",
        "        else:\n",
        "            omega_bound = math.sqrt(c / n) / omega\n",
        "        torch.nn.init.uniform_(l.weight, -omega_bound, omega_bound)\n",
        "        if l.bias is not None:\n",
        "            torch.nn.init.uniform_(l.bias, -omega_bound, omega_bound)\n",
        "\n",
        "\n",
        "class ImageReconINR(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_features: int,\n",
        "        num_layers: int,\n",
        "        out_features: int,\n",
        "        omega=1.0,\n",
        "        omega_0=30.0,\n",
        "        c: float = 6.0,\n",
        "        bias=True,\n",
        "        pos_enc: Optional[Literal[\"positional\"] | Literal[\"gaussian\"]] = None,\n",
        "        sigma: Optional[float] = None,\n",
        "        m_num_freqs: Optional[int] = None,\n",
        "    ):\n",
        "        self._init_kwargs = mrinr.nn.get_module_init_kwargs(\n",
        "            locals(), extra_kwargs_dict=dict()\n",
        "        )\n",
        "        super().__init__()\n",
        "        self.spatial_dims = 3\n",
        "        self.in_features = self.spatial_dims\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.pos_enc_sigma = sigma\n",
        "        self.m_num_freqs = m_num_freqs\n",
        "        if pos_enc == \"positional\":\n",
        "            self.pos_enc = mrinr.nn.PositionalCoordEncoding(\n",
        "                spatial_dims=self.spatial_dims,\n",
        "                sigma_scale=self.pos_enc_sigma,\n",
        "                m_num_freqs=self.m_num_freqs,\n",
        "            )\n",
        "            siren_in_features = self.pos_enc.out_features\n",
        "        elif pos_enc == \"gaussian\":\n",
        "            self.pos_enc = mrinr.nn.GaussianCoordEncodingPreSampled(\n",
        "                spatial_dims=self.spatial_dims,\n",
        "                sigma=self.pos_enc_sigma,\n",
        "                m_num_freqs=self.m_num_freqs,\n",
        "            )\n",
        "            siren_in_features = self.pos_enc.out_features\n",
        "        else:\n",
        "            self.pos_enc = None\n",
        "            siren_in_features = self.spatial_dims\n",
        "\n",
        "        self.siren = Siren(\n",
        "            in_features=siren_in_features,\n",
        "            hidden_features=hidden_features,\n",
        "            out_features=self.out_features,\n",
        "            num_layers=num_layers,\n",
        "            omega=omega,\n",
        "            omega_0=omega_0,\n",
        "            c=c,\n",
        "            bias=bias,\n",
        "        )\n",
        "\n",
        "    def get_extra_state(self) -> Any:\n",
        "        return {\"init_kwargs\": self._init_kwargs}\n",
        "\n",
        "    def set_extra_state(self, state):\n",
        "        return\n",
        "\n",
        "    def _get_reshape_fns(\n",
        "        self,\n",
        "        x: mrinr.typing.CoordGrid3D | torch.Tensor,\n",
        "        grid_fov: torch.Tensor,\n",
        "        grid_min_coord: torch.Tensor,\n",
        "    ) -> tuple[Callable, Callable]:\n",
        "        \"\"\"Get functions to reshape coordinates to/from grids.\n",
        "\n",
        "        Helps with handling inputs that are either full coordinate grids or just\n",
        "        batches of coordinates, the latter of which is useful for Jacobian/Hessian computations.\"\"\"\n",
        "        assert (\n",
        "            x.ndim == grid_fov.ndim == grid_min_coord.ndim\n",
        "        ), f\"Input shapes mismatch: {x.shape =}, {grid_fov.shape =}, {grid_min_coord.shape =}\"\n",
        "        x_orig_shape = tuple(x.shape)\n",
        "        if x.ndim <= 2 and x.shape[-1] == self.spatial_dims:\n",
        "            if x.ndim == 1:\n",
        "                coords_to_grid_reshape = lambda a: einops.repeat(\n",
        "                    a,\n",
        "                    \"coord -> b x y z coord\",\n",
        "                    b=1,\n",
        "                    x=1,\n",
        "                    y=1,\n",
        "                    z=1,\n",
        "                    coord=self.spatial_dims,\n",
        "                )\n",
        "                grid_to_coords_reshape = lambda a: einops.rearrange(\n",
        "                    a,\n",
        "                    \"b x y z coord -> (b x y z) coord\",\n",
        "                    b=1,\n",
        "                    x=1,\n",
        "                    y=1,\n",
        "                    z=1,\n",
        "                    coord=self.spatial_dims,\n",
        "                ).squeeze(0)\n",
        "            else:\n",
        "                coords_to_grid_reshape = lambda a: einops.rearrange(\n",
        "                    a,\n",
        "                    \"(b x y z) coord -> b x y z coord\",\n",
        "                    x=1,\n",
        "                    y=1,\n",
        "                    z=1,\n",
        "                    coord=self.spatial_dims,\n",
        "                )\n",
        "                grid_to_coords_reshape = lambda a: einops.rearrange(\n",
        "                    a,\n",
        "                    \"b x y z coord -> (b x y z) coord\",\n",
        "                    b=x_orig_shape[0],\n",
        "                    coord=self.spatial_dims,\n",
        "                )\n",
        "        else:\n",
        "            # Identity functions.\n",
        "            coords_to_grid_reshape = lambda a: a\n",
        "            grid_to_coords_reshape = lambda a: a\n",
        "        return coords_to_grid_reshape, grid_to_coords_reshape\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: mrinr.typing.CoordGrid3D | torch.Tensor,\n",
        "        grid_fov: torch.Tensor,\n",
        "        grid_min_coord: torch.Tensor,\n",
        "        return_intermediates: bool = False,\n",
        "    ) -> (\n",
        "        mrinr.typing.ScalarVolume\n",
        "        | tuple[mrinr.typing.ScalarVolume, tuple[mrinr.typing.ScalarVolume]]\n",
        "    ):\n",
        "        # print(f\"{x.shape =}, {grid_fov.shape =}, {grid_min_coord.shape =}\")\n",
        "        # Inputs should be 3D coordinate grids, but if they are just a batch of 3D\n",
        "        # coordinates, then reshape to a grid.\n",
        "        coords_to_grid_reshape, _ = self._get_reshape_fns(x, grid_fov, grid_min_coord)\n",
        "        x = coords_to_grid_reshape(x)\n",
        "        grid_fov = coords_to_grid_reshape(grid_fov)\n",
        "        grid_min_coord = coords_to_grid_reshape(grid_min_coord)\n",
        "\n",
        "        # Scale the input coordinates to the range [0, 1] based on the entire volume's\n",
        "        # field of view and lowest coordinate.\n",
        "        # norm_x = (x - grid_min_coord) / grid_fov\n",
        "        # Scale the input coordinates to the range [-1, 1] based on the entire volume's\n",
        "        # field of view and lowest coordinate.\n",
        "        norm_x = (((x - grid_min_coord) / grid_fov) * 2.0) - 1.0\n",
        "\n",
        "        # Positional encoding of input coordinates.\n",
        "        if self.pos_enc is not None:\n",
        "            x_enc = self.pos_enc(norm_x)\n",
        "        else:\n",
        "            x_enc = mrinr.nn.coords_as_channels(norm_x, has_batch_dim=True)\n",
        "        x_enc = einops.rearrange(x_enc, \"b c x y z -> (b x y z) c\")\n",
        "\n",
        "        if return_intermediates:\n",
        "            y, inters = self.siren(x_enc, return_intermediates=True)\n",
        "            y_intermediates = list()\n",
        "            for a in inters:\n",
        "                a = einops.rearrange(\n",
        "                    a,\n",
        "                    \"(b x y z) c -> b c x y z\",\n",
        "                    x=x.shape[1],\n",
        "                    y=x.shape[2],\n",
        "                    z=x.shape[3],\n",
        "                )\n",
        "                y_intermediates.append(a)\n",
        "            y_intermediates = tuple(y_intermediates)\n",
        "        else:\n",
        "            y = self.siren(x_enc)\n",
        "            y_intermediates = None\n",
        "        # Reshape back into a volume, and return.\n",
        "        y = einops.rearrange(\n",
        "            y,\n",
        "            \"(b x y z) c -> b c x y z\",\n",
        "            x=x.shape[1],\n",
        "            y=x.shape[2],\n",
        "            z=x.shape[3],\n",
        "        )\n",
        "\n",
        "        if return_intermediates:\n",
        "            r = y, y_intermediates\n",
        "        else:\n",
        "            r = y\n",
        "        return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dfc46af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EPINR SIREN model with optional positional encoding and rigid affine layer.\n",
        "\n",
        "\n",
        "class EPINR(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_features: int,\n",
        "        num_layers: int,\n",
        "        pe_dir: str,\n",
        "        omega=1.0,\n",
        "        omega_0=30.0,\n",
        "        c: float = 6.0,\n",
        "        non_rigid_affine: bool = False,\n",
        "        non_rigid_affine_nonzero_thresh: Optional[float] = None,\n",
        "        refine_fixed_rigid_affine: bool = False,\n",
        "        bias=True,\n",
        "        pos_enc: Optional[Literal[\"positional\"] | Literal[\"gaussian\"]] = None,\n",
        "        sigma: Optional[float] = None,\n",
        "        m_num_freqs: Optional[int] = None,\n",
        "    ):\n",
        "        self._init_kwargs = mrinr.nn.get_module_init_kwargs(\n",
        "            locals(), extra_kwargs_dict=dict()\n",
        "        )\n",
        "        super().__init__()\n",
        "        self.spatial_dims = 3\n",
        "        self.in_features = self.spatial_dims\n",
        "        self.pe_dir = pe_dir\n",
        "        if self.pe_dir not in {\"ap\", \"pa\"}:\n",
        "            raise NotImplementedError(f\"Unsupported pe_dir: {self.pe_dir}\")\n",
        "\n",
        "        if non_rigid_affine:\n",
        "            self.nonrigid_affine_layer = self._NonRigidAffineLayer(\n",
        "                pe_dir=self.pe_dir,\n",
        "                param_softshrink_lambda=non_rigid_affine_nonzero_thresh,\n",
        "            )\n",
        "        else:\n",
        "            self.nonrigid_affine_layer = None\n",
        "\n",
        "        self.out_features = 1\n",
        "        self.pos_enc_sigma = sigma\n",
        "        self.m_num_freqs = m_num_freqs\n",
        "        if pos_enc == \"positional\":\n",
        "            self.pos_enc = mrinr.nn.PositionalCoordEncoding(\n",
        "                spatial_dims=self.spatial_dims,\n",
        "                sigma_scale=self.pos_enc_sigma,\n",
        "                m_num_freqs=self.m_num_freqs,\n",
        "            )\n",
        "            siren_in_features = self.pos_enc.out_features\n",
        "        elif pos_enc == \"gaussian\":\n",
        "            self.pos_enc = mrinr.nn.GaussianCoordEncodingPreSampled(\n",
        "                spatial_dims=self.spatial_dims,\n",
        "                sigma=self.pos_enc_sigma,\n",
        "                m_num_freqs=self.m_num_freqs,\n",
        "            )\n",
        "            siren_in_features = self.pos_enc.out_features\n",
        "        else:\n",
        "            self.pos_enc = None\n",
        "            siren_in_features = self.spatial_dims\n",
        "\n",
        "        self.siren = Siren(\n",
        "            in_features=siren_in_features,\n",
        "            hidden_features=hidden_features,\n",
        "            out_features=self.out_features,\n",
        "            num_layers=num_layers,\n",
        "            omega=omega,\n",
        "            omega_0=omega_0,\n",
        "            c=c,\n",
        "            bias=bias,\n",
        "        )\n",
        "        #!TESTING\n",
        "        # Initialize the final layer weights to be very small to start as a small\n",
        "        # displacement field.\n",
        "        with torch.no_grad():\n",
        "            final_layer_init_range = 0.0001  # 0.000001\n",
        "            torch.nn.init.uniform_(\n",
        "                self.siren.linear_layers[-1].linear.weight,\n",
        "                -final_layer_init_range,\n",
        "                final_layer_init_range,\n",
        "            )\n",
        "        #!\n",
        "        if refine_fixed_rigid_affine:\n",
        "            self.fixed_rigid_affine = self._RigidAffineLayer()\n",
        "        else:\n",
        "            self.fixed_rigid_affine = None\n",
        "\n",
        "    class _RigidAffineLayer(torch.nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.spatial_dims = 3\n",
        "            self.theta = torch.nn.Parameter(torch.zeros(self.spatial_dims))\n",
        "            self.translations = torch.nn.Parameter(torch.zeros(self.spatial_dims))\n",
        "\n",
        "        def get_affine(self, homogeneous: bool = False):\n",
        "            R_x = torch.eye(self.spatial_dims).to(self.theta)\n",
        "            # [1, 0, 0],\n",
        "            # [0, torch.cos(self.theta[0]), -torch.sin(self.theta[0])],\n",
        "            # [0, torch.sin(self.theta[0]), torch.cos(self.theta[0])],\n",
        "            R_x[1, 1] *= torch.cos(self.theta[0])\n",
        "            R_x[1, 2] += -torch.sin(self.theta[0])\n",
        "            R_x[2, 1] += torch.sin(self.theta[0])\n",
        "            R_x[2, 2] *= torch.cos(self.theta[0])\n",
        "\n",
        "            R_y = torch.eye(self.spatial_dims).to(self.theta)\n",
        "            # [torch.cos(self.theta[1]), 0, torch.sin(self.theta[1])],\n",
        "            # [0, 1, 0],\n",
        "            # [-torch.sin(self.theta[1]), 0, torch.cos(self.theta[1])]\n",
        "            R_y[0, 0] *= torch.cos(self.theta[1])\n",
        "            R_y[0, 2] += torch.sin(self.theta[1])\n",
        "            R_y[2, 0] += -torch.sin(self.theta[1])\n",
        "            R_y[2, 2] *= torch.cos(self.theta[1])\n",
        "\n",
        "            R_z = torch.eye(self.spatial_dims).to(self.theta)\n",
        "            # [torch.cos(self.theta[2]), -torch.sin(self.theta[2]), 0],\n",
        "            # [torch.sin(self.theta[2]), torch.cos(self.theta[2]), 0],\n",
        "            # [0, 0, 1],\n",
        "            R_z[0, 0] *= torch.cos(self.theta[2])\n",
        "            R_z[0, 1] += -torch.sin(self.theta[2])\n",
        "            R_z[1, 0] += torch.sin(self.theta[2])\n",
        "            R_z[1, 1] *= torch.cos(self.theta[2])\n",
        "\n",
        "            R = R_x @ R_y @ R_z\n",
        "            if homogeneous:\n",
        "                A = torch.eye(\n",
        "                    self.spatial_dims + 1,\n",
        "                    device=self.translations.device,\n",
        "                    dtype=self.translations.dtype,\n",
        "                )\n",
        "                A[: self.spatial_dims, : self.spatial_dims] = R\n",
        "                A[:-1, -1] = self.translations\n",
        "                r = A\n",
        "            else:\n",
        "                r = R, self.translations\n",
        "            return r\n",
        "\n",
        "        def forward(self, x):\n",
        "            R, t = self.get_affine(homogeneous=False)\n",
        "            y = einops.einsum(x, R, \"... i, i j -> ... j\") + t\n",
        "            # linear() requires a transposed matrix.\n",
        "            # y = torch.nn.functional.linear(x, weight=R.T, bias=self.translations)\n",
        "            return y\n",
        "\n",
        "    class _NonRigidAffineLayer(torch.nn.Module):\n",
        "        def __init__(\n",
        "            self,\n",
        "            pe_dir: str,\n",
        "            param_softshrink_lambda: Optional[float] = None,\n",
        "        ):\n",
        "            super().__init__()\n",
        "            self.spatial_dims = 3\n",
        "            self.pe_dir = pe_dir\n",
        "            if self.pe_dir not in {\"ap\", \"pa\"}:\n",
        "                raise NotImplementedError(f\"Unsupported pe_dir: {self.pe_dir}\")\n",
        "            self.scales = torch.nn.Parameter(torch.ones(self.spatial_dims))\n",
        "            self.shears = torch.nn.Parameter(torch.zeros(self.spatial_dims))\n",
        "            if (param_softshrink_lambda is None) or (param_softshrink_lambda == 0.0):\n",
        "                self.param_nonlinear = None\n",
        "            else:\n",
        "                self.param_nonlinear = torch.nn.Softshrink(\n",
        "                    lambd=param_softshrink_lambda\n",
        "                )\n",
        "\n",
        "        def get_affine(self, homogeneous: bool = False):\n",
        "            # Apply nonlinear to parameters so they remain at identity when only small\n",
        "            # updates are made, ensuring that large gradients are needed to \"activate\"\n",
        "            # the affine transformation.\n",
        "            if self.param_nonlinear is not None:\n",
        "                scales = self.param_nonlinear(self.scales - 1.0) + 1.0\n",
        "                shears = self.param_nonlinear(self.shears)\n",
        "            else:\n",
        "                scales = self.scales\n",
        "                shears = self.shears\n",
        "            # Scaling matrix.\n",
        "            S = torch.diag(scales)\n",
        "            # Shear matrix.\n",
        "            P = torch.eye(self.spatial_dims, dtype=shears.dtype, device=shears.device)\n",
        "            P[0, 1] = shears[0]  # shear xy\n",
        "            # shear xz, which is 0 for AP/PA PE directions.\n",
        "            P[0, 2] = shears[1] * 0.0\n",
        "            P[1, 2] = shears[2]  # shear yz\n",
        "\n",
        "            A = S @ P\n",
        "            if homogeneous:\n",
        "                A = torch.nn.functional.pad(A, pad=(0, 1, 0, 1), value=0.0)\n",
        "                A[-1, -1] = 1.0\n",
        "            return A\n",
        "\n",
        "    def get_extra_state(self) -> Any:\n",
        "        return {\"init_kwargs\": self._init_kwargs}\n",
        "\n",
        "    def set_extra_state(self, state):\n",
        "        return\n",
        "\n",
        "    def fixed2moving_rigid_affine_parameters(self):\n",
        "        if self.fixed_rigid_affine is not None:\n",
        "            return self.fixed_rigid_affine.parameters()\n",
        "        else:\n",
        "            return list()\n",
        "\n",
        "    def moving2fixed_nonrigid_affine_parameters(self):\n",
        "        if self.nonrigid_affine_layer is not None:\n",
        "            return self.nonrigid_affine_layer.parameters()\n",
        "        else:\n",
        "            return list()\n",
        "\n",
        "    def network_parameters(self):\n",
        "        p_iter_all = self.parameters()\n",
        "        fix2mov_rigid_params = list(self.fixed2moving_rigid_affine_parameters())\n",
        "        mov2fix_nonrigid_params = list(self.moving2fixed_nonrigid_affine_parameters())\n",
        "        affine_params = set(fix2mov_rigid_params + mov2fix_nonrigid_params)\n",
        "        net_params = [p for p in p_iter_all if p not in affine_params]\n",
        "        return net_params\n",
        "\n",
        "    @property\n",
        "    def fixed2moving_rigid_affine(\n",
        "        self,\n",
        "    ) -> mrinr.typing.SingleHomogeneousAffine3D | None:\n",
        "        if self.fixed_rigid_affine is not None:\n",
        "            A = self.fixed_rigid_affine.get_affine(homogeneous=True)\n",
        "        else:\n",
        "            A = None\n",
        "        return A\n",
        "\n",
        "    def apply_fixed_rigid_affine(\n",
        "        self, x: mrinr.typing.CoordGrid3D\n",
        "    ) -> mrinr.typing.CoordGrid3D:\n",
        "        if self.fixed_rigid_affine is not None:\n",
        "            y = self.fixed_rigid_affine(x)\n",
        "        else:\n",
        "            y = x\n",
        "        return y\n",
        "\n",
        "    def _get_reshape_fns(\n",
        "        self,\n",
        "        x: mrinr.typing.CoordGrid3D | torch.Tensor,\n",
        "        grid_fov: torch.Tensor,\n",
        "        grid_min_coord: torch.Tensor,\n",
        "    ) -> tuple[Callable, Callable]:\n",
        "        \"\"\"Get functions to reshape coordinates to/from grids.\n",
        "\n",
        "        Helps with handling inputs that are either full coordinate grids or just\n",
        "        batches of coordinates, the latter of which is useful for Jacobian/Hessian computations.\"\"\"\n",
        "        assert (\n",
        "            x.ndim == grid_fov.ndim == grid_min_coord.ndim\n",
        "        ), f\"Input shapes mismatch: {x.shape =}, {grid_fov.shape =}, {grid_min_coord.shape =}\"\n",
        "        x_orig_shape = tuple(x.shape)\n",
        "        if x.ndim <= 2 and x.shape[-1] == self.spatial_dims:\n",
        "            if x.ndim == 1:\n",
        "                coords_to_grid_reshape = lambda a: einops.repeat(\n",
        "                    a,\n",
        "                    \"coord -> b x y z coord\",\n",
        "                    b=1,\n",
        "                    x=1,\n",
        "                    y=1,\n",
        "                    z=1,\n",
        "                    coord=self.spatial_dims,\n",
        "                )\n",
        "                grid_to_coords_reshape = lambda a: einops.rearrange(\n",
        "                    a,\n",
        "                    \"b x y z coord -> (b x y z) coord\",\n",
        "                    b=1,\n",
        "                    x=1,\n",
        "                    y=1,\n",
        "                    z=1,\n",
        "                    coord=self.spatial_dims,\n",
        "                ).squeeze(0)\n",
        "            else:\n",
        "                coords_to_grid_reshape = lambda a: einops.rearrange(\n",
        "                    a,\n",
        "                    \"(b x y z) coord -> b x y z coord\",\n",
        "                    x=1,\n",
        "                    y=1,\n",
        "                    z=1,\n",
        "                    coord=self.spatial_dims,\n",
        "                )\n",
        "                grid_to_coords_reshape = lambda a: einops.rearrange(\n",
        "                    a,\n",
        "                    \"b x y z coord -> (b x y z) coord\",\n",
        "                    b=x_orig_shape[0],\n",
        "                    coord=self.spatial_dims,\n",
        "                )\n",
        "        else:\n",
        "            # Identity functions.\n",
        "            coords_to_grid_reshape = lambda a: a\n",
        "            grid_to_coords_reshape = lambda a: a\n",
        "        return coords_to_grid_reshape, grid_to_coords_reshape\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: mrinr.typing.CoordGrid3D | torch.Tensor,\n",
        "        grid_fov: torch.Tensor,\n",
        "        grid_min_coord: torch.Tensor,\n",
        "        _scale_by_fov: bool = True,\n",
        "        _add_nonrigid: bool = True,\n",
        "    ) -> mrinr.typing.CoordGrid3D:\n",
        "        # print(f\"{x.shape =}, {grid_fov.shape =}, {grid_min_coord.shape =}\")\n",
        "        # Inputs should be 3D coordinate grids, but if they are just a batch of 3D\n",
        "        # coordinates, then reshape to a grid.\n",
        "        coords_to_grid_reshape, grid_to_coords_reshape = self._get_reshape_fns(\n",
        "            x, grid_fov, grid_min_coord\n",
        "        )\n",
        "        x = coords_to_grid_reshape(x)\n",
        "        grid_fov = coords_to_grid_reshape(grid_fov)\n",
        "        grid_min_coord = coords_to_grid_reshape(grid_min_coord)\n",
        "\n",
        "        # Use a learned non-rigid affine transformation to create a secondary displacement\n",
        "        # field.\n",
        "        if self.nonrigid_affine_layer is not None:\n",
        "            # Broadcast batch dim.\n",
        "            A = self.nonrigid_affine_layer.get_affine(homogeneous=True).unsqueeze(0)\n",
        "            Ax = mrinr.coords.transform_coords(x, affine_a2b=A, broadcast_batch=True)\n",
        "            y_nonrigid = Ax - x\n",
        "        else:\n",
        "            y_nonrigid = None\n",
        "\n",
        "        # Scale the input coordinates to the range [-1, 1] based on the entire volume's\n",
        "        # field of view and lowest coordinate.\n",
        "        norm_x = (((x - grid_min_coord) / grid_fov) * 2.0) - 1.0\n",
        "\n",
        "        # Positional encoding of input coordinates.\n",
        "        if self.pos_enc is not None:\n",
        "            x_enc = self.pos_enc(norm_x)\n",
        "        else:\n",
        "            x_enc = mrinr.nn.coords_as_channels(norm_x, has_batch_dim=True)\n",
        "        x_enc = einops.rearrange(x_enc, \"b c x y z -> (b x y z) c\")\n",
        "        y = self.siren(x_enc)\n",
        "        # Reshape back into a coordinate grid, and return.\n",
        "        y = einops.rearrange(\n",
        "            y,\n",
        "            \"(b x y z) coord -> b x y z coord\",\n",
        "            x=x.shape[1],\n",
        "            y=x.shape[2],\n",
        "            z=x.shape[3],\n",
        "        )\n",
        "\n",
        "        y = torch.cat([torch.zeros_like(y), y, torch.zeros_like(y)], dim=-1)\n",
        "        # Unscale the output coordinates to the grid.\n",
        "        # y = (y * grid_fov) + grid_min_coord\n",
        "        if _scale_by_fov:\n",
        "            y = y * grid_fov\n",
        "        if y_nonrigid is not None and _add_nonrigid:\n",
        "            y = y + y_nonrigid\n",
        "        y = grid_to_coords_reshape(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01c19706",
      "metadata": {},
      "source": [
        "## Data Processing & Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96cea0a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def prep_vols(\n",
        "    b0,\n",
        "    b0_mask,\n",
        "    b0_affine,\n",
        "    t1w,\n",
        "    t1w_mask,\n",
        "    t1w_affine,\n",
        "    scale_kwargs: dict,\n",
        "    blur_mask_kwargs: dict,\n",
        ") -> dict:\n",
        "    # Scale the volumes to have similar intensity ranges.\n",
        "    b0 = scale_vol(b0, **scale_kwargs)\n",
        "    t1w = scale_vol(t1w, **scale_kwargs)\n",
        "    # Blur masks to focus training loss on brain tissue instead of skull/background.\n",
        "    b0_spacing = mrinr.coords.spacing(b0_affine)\n",
        "    b0_mask = blur_mask(b0_mask, spacing=b0_spacing, **blur_mask_kwargs)\n",
        "    t1w_spacing = mrinr.coords.spacing(t1w_affine)\n",
        "    t1w_mask = blur_mask(t1w_mask, spacing=t1w_spacing, **blur_mask_kwargs)\n",
        "    vols = dict(\n",
        "        b0=b0,\n",
        "        b0_mask=b0_mask,\n",
        "        t1w=t1w,\n",
        "        t1w_mask=t1w_mask,\n",
        "    )\n",
        "    return vols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d89a64",
      "metadata": {},
      "outputs": [],
      "source": [
        "class RepeatVolPatchDataset(monai.data.IterableDataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vol_data: dict,\n",
        "        n_repeats: int,\n",
        "        starting_epoch: int = 1,\n",
        "        **patch_iterd_kwargs,\n",
        "    ):\n",
        "        # See <https://docs.monai.io/en/stable/data.html#patchiterd> for PatchIterd\n",
        "        # kwargs.\n",
        "        self._data = vol_data\n",
        "        self.n_repeats = n_repeats\n",
        "        self._starting_epoch = starting_epoch\n",
        "        self._epoch = self._starting_epoch\n",
        "        self._patch_iterd_kwargs = patch_iterd_kwargs\n",
        "        ex_iter = monai.data.PatchIterd(**self._patch_iterd_kwargs)\n",
        "        self.n_patches_in_vol = len(list(ex_iter(self._data)))\n",
        "\n",
        "    def __iter__(self):\n",
        "        info = torch.utils.data.get_worker_info()\n",
        "        num_workers = info.num_workers if info is not None else 1\n",
        "        id = info.id if info is not None else 0\n",
        "\n",
        "        for i in range(self.n_repeats):\n",
        "            if i % num_workers == id:\n",
        "                for j, (item, coords) in enumerate(\n",
        "                    monai.data.PatchIterd(**self._patch_iterd_kwargs)(self._data)\n",
        "                ):\n",
        "                    yield item\n",
        "        self._epoch += 1\n",
        "\n",
        "\n",
        "class RandSingleVolCoordSampleDataset(\n",
        "    monai.data.iterable_dataset.Randomizable, monai.data.IterableDataset\n",
        "):\n",
        "    def __init__(\n",
        "        self,\n",
        "        coord_grid: mrinr.typing.SingleCoordGrid3D,\n",
        "        weight_mask: mrinr.typing.SingleScalarVolume,\n",
        "        n_samples_per_batch: int,\n",
        "        n_batches_per_epoch: int,\n",
        "        seed: int = 1,\n",
        "        p_rand_coord_shift: float = 0.0,\n",
        "        rand_shift_ranges: Optional[tuple[float, float, float]] = None,\n",
        "        starting_epoch: int = 1,\n",
        "    ):\n",
        "        super().__init__(data=[coord_grid], transform=None)\n",
        "        self._coords = coord_grid\n",
        "        self._coords_as_vol = mrinr.nn.coords_as_channels(\n",
        "            self._coords, has_batch_dim=False\n",
        "        )\n",
        "\n",
        "        self._weight_mask = weight_mask\n",
        "        self._cumul_weight_mask = mrinr.data.cumul_sampling_mask_from_weight_mask(\n",
        "            weight_mask=weight_mask\n",
        "        )\n",
        "        self.n_samples_per_batch = n_samples_per_batch\n",
        "        self.n_batches_per_epoch = n_batches_per_epoch\n",
        "        self._starting_epoch = starting_epoch\n",
        "        self._epoch = self._starting_epoch\n",
        "\n",
        "        self.seed = seed\n",
        "        self.p_rand_coord_shift = p_rand_coord_shift\n",
        "        self.rand_shift_ranges = rand_shift_ranges\n",
        "        if isinstance(self.rand_shift_ranges, float):\n",
        "            self.rand_shift_ranges = [self.rand_shift_ranges] * 3\n",
        "        if self.p_rand_coord_shift > 0.0 and self.rand_shift_ranges is None:\n",
        "            raise ValueError(\n",
        "                \"If p_rand_coord_shift > 0.0, rand_shift_ranges must be provided.\"\n",
        "            )\n",
        "        if self.rand_shift_ranges is not None:\n",
        "            self.rand_shift_ranges = tuple(self.rand_shift_ranges)\n",
        "            if len(self.rand_shift_ranges) != 3:\n",
        "                raise ValueError(\"rand_shift_ranges must be a tuple of 3 floats.\")\n",
        "        self.R.seed(self.seed)\n",
        "        self.random_vars = None\n",
        "        self.rand_coord_shifts = None\n",
        "\n",
        "    def randomize(self, size: tuple) -> None:\n",
        "        self.random_vars = torch.from_numpy(self.R.random_sample(size)).to(\n",
        "            self._cumul_weight_mask\n",
        "        )\n",
        "        if self.p_rand_coord_shift > 0.0:\n",
        "            do_rand_shift = self.R.random_sample(size) < self.p_rand_coord_shift\n",
        "            rand_shifts = list()\n",
        "            for dim in range(3):\n",
        "                shift_range = self.rand_shift_ranges[dim]\n",
        "                shifts_dim = self.R.uniform(\n",
        "                    -shift_range, shift_range, size=size\n",
        "                ).astype(np.float32)\n",
        "                rand_shifts.append(shifts_dim)\n",
        "            rand_shifts = np.stack(rand_shifts, axis=-1)\n",
        "            rand_shifts = rand_shifts * do_rand_shift[..., None]\n",
        "            self.rand_coord_shifts = torch.from_numpy(rand_shifts).to(\n",
        "                self._cumul_weight_mask\n",
        "            )\n",
        "        else:\n",
        "            self.rand_coord_shifts = None\n",
        "\n",
        "    def get_random_coords_from_rv(\n",
        "        self, random_vars: torch.Tensor, coord_shifts: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        rv = random_vars.view(-1)\n",
        "        patch_center_flat_indices = torch.searchsorted(\n",
        "            self._cumul_weight_mask.view(-1), rv, side=\"right\"\n",
        "        ).clamp_max_(self._cumul_weight_mask.numel() - 1)\n",
        "        patch_center_indices = np.unravel_index(\n",
        "            patch_center_flat_indices.cpu().numpy(),\n",
        "            shape=tuple(self._cumul_weight_mask.shape),\n",
        "        )\n",
        "        patch_center_el_coords = torch.from_numpy(\n",
        "            np.asarray(patch_center_indices)\n",
        "        ).T.to(self._cumul_weight_mask.device)\n",
        "        # Ensure coords are within valid range.\n",
        "        max_idx = patch_center_el_coords.new_tensor(self._coords_as_vol.shape[1:]) - 1\n",
        "        patch_center_el_coords = torch.minimum(\n",
        "            patch_center_el_coords, max_idx.reshape(1, 3)\n",
        "        )\n",
        "\n",
        "        coord_samples = self._coords_as_vol[\n",
        "            :,\n",
        "            patch_center_el_coords[:, 0],\n",
        "            patch_center_el_coords[:, 1],\n",
        "            patch_center_el_coords[:, 2],\n",
        "        ]\n",
        "        coord_samples = einops.rearrange(coord_samples, \"coord b -> b coord 1 1 1\")\n",
        "        if coord_shifts is not None:\n",
        "            shifts = einops.rearrange(coord_shifts, \"b coord -> b coord 1 1 1\")\n",
        "            coord_samples = coord_samples + shifts\n",
        "        return coord_samples\n",
        "\n",
        "    def __iter__(self):\n",
        "        info = torch.utils.data.get_worker_info()\n",
        "        num_workers = info.num_workers if info is not None else 1\n",
        "        id = info.id if info is not None else 0\n",
        "\n",
        "        for i in range(self.n_batches_per_epoch):\n",
        "            # Sample locations one batch at a time.\n",
        "            self.randomize(self.n_samples_per_batch)\n",
        "            if i % num_workers == id:\n",
        "                rand_coords = self.get_random_coords_from_rv(self.random_vars)\n",
        "                yield rand_coords\n",
        "\n",
        "        self._epoch += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2cc11e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pretrain_warped_atlas(\n",
        "    epinr: EPINR,\n",
        "    max_iters: int,\n",
        "    subj_data: DWISubjectData,\n",
        "    params: Box,\n",
        "    optim: torch.optim.Optimizer,\n",
        "    batch_size: int,\n",
        "    pr=print,\n",
        ") -> dict:\n",
        "    y = subj_data.suscept_atlas_mm\n",
        "    assert y.shape[1] == 1\n",
        "    y_mask = y > 0.0\n",
        "    # B x 3\n",
        "    coords = subj_data.b0_scanner_coord_grid[y_mask.squeeze(1)]\n",
        "    grid_fov = subj_data.b0_fov\n",
        "    grid_min_coord = subj_data.b0_min_coord\n",
        "    grid_fov = einops.rearrange(\n",
        "        grid_fov,\n",
        "        \"b x y z coord -> (b x y z) coord\",\n",
        "        coord=3,\n",
        "    )\n",
        "    grid_min_coord = einops.rearrange(\n",
        "        grid_min_coord,\n",
        "        \"b x y z coord -> (b x y z) coord\",\n",
        "        coord=3,\n",
        "    )\n",
        "    # B x 1\n",
        "    y = y[y_mask].unsqueeze(-1)\n",
        "    # Append 0s for x and z coordinates since susceptibility is only along y.\n",
        "    y = torch.cat(\n",
        "        [\n",
        "            torch.zeros_like(y),\n",
        "            y,\n",
        "            torch.zeros_like(y),\n",
        "        ],\n",
        "        dim=-1,\n",
        "    )\n",
        "\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    train_losses = dict(epoch=list(), loss=list())\n",
        "\n",
        "    pr(\"Pretraining INR to susceptibility atlas warped to subject space...\")\n",
        "    log_epochs = set(\n",
        "        [1] + np.linspace(1, max_iters + 1, num=10, dtype=int).tolist() + [max_iters]\n",
        "    )\n",
        "    epinr.train()\n",
        "    optim.zero_grad()\n",
        "    for epoch in range(1, max_iters + 1):\n",
        "        pred_y = epinr(\n",
        "            coords,\n",
        "            grid_fov=grid_fov,\n",
        "            grid_min_coord=grid_min_coord,\n",
        "        )\n",
        "        loss = loss_fn(pred_y, y)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        train_losses[\"epoch\"].append(epoch)\n",
        "        train_losses[\"loss\"].append(loss.detach().item())\n",
        "        if epoch in log_epochs:\n",
        "            pr(f\"Epoch {epoch}/{max_iters}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "    pr(\"Pretraining complete.\")\n",
        "    optim.zero_grad()\n",
        "    epinr.eval()\n",
        "    with torch.no_grad():\n",
        "        # Break up the prediction into smaller chunks to avoid OOM errors.\n",
        "        n_chunks = 16\n",
        "        b0_coord_grid_chunks = torch.chunk(\n",
        "            subj_data.b0_scanner_coord_grid, n_chunks, dim=-2\n",
        "        )\n",
        "        pred_displacement_field_mm_chunks = list()\n",
        "        J_chunks = list()\n",
        "        J_deform_chunks = list()\n",
        "        for b0_coord_grid_chunk in b0_coord_grid_chunks:\n",
        "            pred_displacement_field_mm_chunk = epinr(\n",
        "                x=b0_coord_grid_chunk,\n",
        "                grid_fov=subj_data.b0_fov,\n",
        "                grid_min_coord=subj_data.b0_min_coord,\n",
        "            )\n",
        "            pred_displacement_field_mm_chunks.append(pred_displacement_field_mm_chunk)\n",
        "            J_chunk = jacobian_matrix_batched(\n",
        "                pred_displacement_net=epinr,\n",
        "                x_coords=b0_coord_grid_chunk,\n",
        "                grid_fov=subj_data.b0_fov,\n",
        "                grid_min_coord=subj_data.b0_min_coord,\n",
        "                jac_of_warp_type=\"displacement\",\n",
        "            )\n",
        "            J_chunks.append(J_chunk)\n",
        "            J_deform_chunk = jacobian_matrix_batched(\n",
        "                pred_displacement_net=epinr,\n",
        "                x_coords=b0_coord_grid_chunk,\n",
        "                grid_fov=subj_data.b0_fov,\n",
        "                grid_min_coord=subj_data.b0_min_coord,\n",
        "                jac_of_warp_type=\"deformation\",\n",
        "            )\n",
        "            J_deform_chunks.append(J_deform_chunk)\n",
        "        pred_displacement_field_mm = torch.cat(\n",
        "            pred_displacement_field_mm_chunks, dim=-2\n",
        "        )\n",
        "        J = torch.cat(J_chunks, dim=-3)\n",
        "        det_J_ap = 1 + J[:, None, ..., 1, 1]\n",
        "        # Sample the moving b0 volume with the predicted displacement field.\n",
        "        moving_b0_sample = det_J_ap * mrinr.grid_resample(\n",
        "            subj_data.b0.expand(\n",
        "                subj_data.b0_scanner_coord_grid.shape[0], -1, -1, -1, -1\n",
        "            ),\n",
        "            affine_x_el2coords=subj_data.b0_affine,\n",
        "            sample_coords=subj_data.b0_scanner_coord_grid + pred_displacement_field_mm,\n",
        "            **params.resample_kwargs,\n",
        "        )\n",
        "\n",
        "        # Sample the fixed volume without any affine refinement.\n",
        "        fixed_t1w_sample = mrinr.grid_resample(\n",
        "            subj_data.t1w.expand(\n",
        "                subj_data.b0_scanner_coord_grid.shape[0], -1, -1, -1, -1\n",
        "            ),\n",
        "            affine_x_el2coords=subj_data.t1w_affine,\n",
        "            sample_coords=subj_data.b0_scanner_coord_grid,\n",
        "            **params.resample_kwargs,\n",
        "        )\n",
        "        plt.clf()\n",
        "        plt.figure(dpi=150)\n",
        "        mrinr.viz.plot_1_ch_multi_slice_compare_vols(\n",
        "            subj_data.b0,\n",
        "            moving_b0_sample,\n",
        "            fixed_t1w_sample,\n",
        "            subj_data.suscept_atlas_mm,\n",
        "            pred_displacement_field_mm[..., 1],\n",
        "            det_J_ap,\n",
        "            colorbars=\"rows\",\n",
        "            vol_labels=[\n",
        "                \"Input Distorted b0\",\n",
        "                \"Pred. b0 Undist.\",\n",
        "                \"T1w\",\n",
        "                \"Suscept. Atlas mm\",\n",
        "                \"Pred. Displ. mm Y\",\n",
        "                \"Continuous Det. J\",\n",
        "            ],\n",
        "            axial_slice_idx=(0.35, 0.5),\n",
        "            saggital_slice_idx=0.45,\n",
        "        )\n",
        "        plt.show()\n",
        "\n",
        "    return dict(\n",
        "        epinr=epinr,\n",
        "        optim=optim,\n",
        "        train_losses=train_losses,\n",
        "    )\n",
        "\n",
        "\n",
        "def pretrain_fixed_moving_inrs(\n",
        "    fixed: mrinr.typing.Volume,\n",
        "    fixed_affine: mrinr.typing.HomogeneousAffine3D,\n",
        "    moving: mrinr.typing.Volume,\n",
        "    moving_affine: mrinr.typing.HomogeneousAffine3D,\n",
        "    moving_coord_grid: mrinr.typing.CoordGrid3D,\n",
        "    grid_fov: torch.Tensor,\n",
        "    grid_min_coord: torch.Tensor,\n",
        "    max_iters: int,\n",
        "    lambda_recon: float,\n",
        "    lambda_intermediate_sync: float,\n",
        "    inr_kwargs: dict,\n",
        "    optim_kwargs: dict,\n",
        "    mi_kwargs: dict,\n",
        "    batch_size: int,\n",
        "    patch_size: tuple[int, int, int],\n",
        "    device,\n",
        "    optim_class=torch.optim.AdamW,\n",
        ") -> dict:\n",
        "    assert fixed.shape[1] == moving.shape[1] == 1\n",
        "\n",
        "    # Random patch dataset.\n",
        "    ######\n",
        "    to_be_patched_coord_grid = mrinr.nn.coords_as_channels(\n",
        "        moving_coord_grid.squeeze(0).cpu(), has_batch_dim=False\n",
        "    )\n",
        "    coord_patch_dataset = monai.data.PatchDataset(\n",
        "        # Data is a shallow copy of the same input coordinate grid, one copy for each batch\n",
        "        # in the epoch.\n",
        "        data=[\n",
        "            {\n",
        "                \"coords\": to_be_patched_coord_grid,\n",
        "                \"weight_mask\": to_be_patched_coord_grid.new_ones(\n",
        "                    [1] + list(to_be_patched_coord_grid.shape[1:])\n",
        "                ),\n",
        "            }\n",
        "        ]\n",
        "        * max_iters,\n",
        "        patch_func=monai.transforms.RandWeightedCropDict(\n",
        "            keys=\"coords\",\n",
        "            w_key=\"weight_mask\",\n",
        "            spatial_size=patch_size,\n",
        "            num_samples=batch_size,\n",
        "        ).set_random_state(seed=1),\n",
        "        samples_per_image=batch_size,\n",
        "    )\n",
        "    #######\n",
        "    coord_patch_dataset = RandSingleVolCoordSampleDataset(\n",
        "        coord_grid=moving_coord_grid.squeeze(0).cpu(),\n",
        "        weight_mask=torch.ones(1, *moving_coord_grid.shape[1:-1]),\n",
        "        n_samples_per_batch=batch_size,\n",
        "        n_batches_per_epoch=max_iters,\n",
        "    )\n",
        "    #######\n",
        "    coord_patch_dataloader = monai.data.DataLoader(\n",
        "        coord_patch_dataset,\n",
        "        batch_size=None,\n",
        "        collate_fn=None,\n",
        "        num_workers=3,\n",
        "        prefetch_factor=3,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "    )\n",
        "    ######\n",
        "\n",
        "    # Create initially identical INRs for fixed and moving volumes.\n",
        "    fixed_inr = ImageReconINR(**inr_kwargs).to(device)\n",
        "    moving_inr = ImageReconINR(**inr_kwargs).to(device)\n",
        "    moving_inr.load_state_dict(fixed_inr.state_dict())\n",
        "\n",
        "    optim = optim_class(\n",
        "        [{\"params\": fixed_inr.parameters()}, {\"params\": moving_inr.parameters()}],\n",
        "        **optim_kwargs,\n",
        "    )\n",
        "\n",
        "    recon_loss = torch.nn.MSELoss().to(device)\n",
        "    mi_loss = WeightedNMIParzenLoss(**mi_kwargs, reduction=\"none\").to(device)\n",
        "    train_losses = dict(epoch=list(), network=list(), metric=list(), loss=list())\n",
        "    optim.zero_grad()\n",
        "\n",
        "    print(\"Pretraining fixed and moving INRs...\")\n",
        "    log_epochs = set(\n",
        "        [1] + np.linspace(1, max_iters + 1, num=20, dtype=int).tolist() + [max_iters]\n",
        "    )\n",
        "    fixed_inr.train()\n",
        "    moving_inr.train()\n",
        "    for epoch, coords in enumerate(coord_patch_dataloader, start=1):\n",
        "        coords = mrinr.nn.channels_as_coords(coords, has_batch_dim=True).to(device)\n",
        "        coords = einops.rearrange(coords, \"b x y z coord -> 1 (b x) y z coord\")\n",
        "        f = mrinr.grid_resample(\n",
        "            fixed.expand(coords.shape[0], -1, -1, -1, -1),\n",
        "            affine_x_el2coords=fixed_affine,\n",
        "            sample_coords=coords,\n",
        "            mode_or_interpolation=\"linear\",\n",
        "            padding_mode_or_bound=\"zeros\",\n",
        "            interp_lib=\"torch\",\n",
        "        )\n",
        "        m = mrinr.grid_resample(\n",
        "            moving.expand(coords.shape[0], -1, -1, -1, -1),\n",
        "            affine_x_el2coords=moving_affine,\n",
        "            sample_coords=coords,\n",
        "            mode_or_interpolation=\"linear\",\n",
        "            padding_mode_or_bound=\"zeros\",\n",
        "            interp_lib=\"torch\",\n",
        "        )\n",
        "\n",
        "        pred_f, f_intermediates = fixed_inr(\n",
        "            coords,\n",
        "            grid_fov=grid_fov,\n",
        "            grid_min_coord=grid_min_coord,\n",
        "            return_intermediates=True,\n",
        "        )\n",
        "        pred_m, m_intermediates = moving_inr(\n",
        "            coords,\n",
        "            grid_fov=grid_fov,\n",
        "            grid_min_coord=grid_min_coord,\n",
        "            return_intermediates=True,\n",
        "        )\n",
        "        loss_fixed_recon = recon_loss(pred_f, f)\n",
        "        loss_moving_recon = recon_loss(pred_m, m)\n",
        "        if lambda_intermediate_sync > 0.0:\n",
        "            mi_intermediate_loss = list()\n",
        "            for f_int, m_int in zip(f_intermediates, m_intermediates):\n",
        "                # Sum over the channel dimension, should be shape (B,)\n",
        "                mi_intermediate_loss.append(\n",
        "                    mi_loss(\n",
        "                        f_int,\n",
        "                        m_int,\n",
        "                    ).mean(1)\n",
        "                )\n",
        "            mi_intermediate_loss = torch.stack(mi_intermediate_loss, dim=0).mean()\n",
        "        else:\n",
        "            mi_intermediate_loss = torch.tensor(0.0).to(device)\n",
        "        loss_fixed = (lambda_recon * loss_fixed_recon) + (\n",
        "            lambda_intermediate_sync * mi_intermediate_loss\n",
        "        )\n",
        "        loss_moving = (lambda_recon * loss_moving_recon) + (\n",
        "            lambda_intermediate_sync * mi_intermediate_loss\n",
        "        )\n",
        "\n",
        "        loss_fixed.backward(retain_graph=True)\n",
        "        loss_moving.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        for network_name, metric_name, l in zip(\n",
        "            [\"fixed_inr\", \"moving_inr\"] * 2,\n",
        "            [\"recon\"] * 2 + [\"intermediate_mi\"] * 2,\n",
        "            [\n",
        "                lambda_recon * loss_fixed_recon.detach().item(),\n",
        "                lambda_recon * loss_moving_recon.detach().item(),\n",
        "                lambda_intermediate_sync * mi_intermediate_loss.detach().item(),\n",
        "                lambda_intermediate_sync * mi_intermediate_loss.detach().item(),\n",
        "            ],\n",
        "        ):\n",
        "            train_losses[\"epoch\"].append(epoch)\n",
        "            train_losses[\"network\"].append(network_name)\n",
        "            train_losses[\"metric\"].append(metric_name)\n",
        "            train_losses[\"loss\"].append(l)\n",
        "\n",
        "        if epoch in log_epochs:\n",
        "            print(\n",
        "                f\"Epoch {epoch}/{max_iters},\"\n",
        "                f\"Fixed recon loss: {lambda_recon * loss_fixed_recon.detach().item():.6f}, \"\n",
        "                f\"Moving recon loss: {lambda_recon * loss_moving_recon.detach().item():.6f}, \"\n",
        "                f\"Intermediate MI loss: {lambda_intermediate_sync * mi_intermediate_loss.detach().item():.6f}\"\n",
        "            )\n",
        "\n",
        "    optim.zero_grad()\n",
        "    print(\"Pretraining complete.\")\n",
        "\n",
        "    n_chunks = 16\n",
        "    moving_coord_grid_chunks = torch.chunk(moving_coord_grid, n_chunks, dim=-2)\n",
        "    pred_fixed_chunks = list()\n",
        "    pred_moving_chunks = list()\n",
        "    with torch.no_grad():\n",
        "        for moving_coord_grid_chunk in moving_coord_grid_chunks:\n",
        "            pred_f_chunk = fixed_inr(\n",
        "                moving_coord_grid_chunk,\n",
        "                grid_fov=grid_fov,\n",
        "                grid_min_coord=grid_min_coord,\n",
        "            )\n",
        "            pred_fixed_chunks.append(pred_f_chunk.cpu())\n",
        "            pred_m_chunk = moving_inr(\n",
        "                moving_coord_grid_chunk,\n",
        "                grid_fov=grid_fov,\n",
        "                grid_min_coord=grid_min_coord,\n",
        "            )\n",
        "            pred_moving_chunks.append(pred_m_chunk.cpu())\n",
        "    pred_fixed = torch.cat(pred_fixed_chunks, dim=-1)\n",
        "    pred_moving = torch.cat(pred_moving_chunks, dim=-1)\n",
        "\n",
        "    plt.clf()\n",
        "    plt.figure(dpi=250)\n",
        "    mrinr.viz.plot_1_ch_multi_slice_compare_vols(\n",
        "        fixed[0],\n",
        "        pred_fixed,\n",
        "        moving[0],\n",
        "        pred_moving,\n",
        "        vol_labels=[\n",
        "            \"Fixed Volume\",\n",
        "            \"Fixed INR Recon\",\n",
        "            \"Moving Volume\",\n",
        "            \"Moving INR Recon\",\n",
        "        ],\n",
        "        colorbars=\"global\",\n",
        "        axial_slice_idx=(0.35, 0.5),\n",
        "        saggital_slice_idx=0.45,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    return dict(fixed_inr=fixed_inr, moving_inr=moving_inr, train_losses=train_losses)\n",
        "\n",
        "\n",
        "def pretrain_sequential_fixed_moving_inrs(\n",
        "    fixed: mrinr.typing.Volume,\n",
        "    fixed_affine: mrinr.typing.HomogeneousAffine3D,\n",
        "    moving: mrinr.typing.Volume,\n",
        "    moving_affine: mrinr.typing.HomogeneousAffine3D,\n",
        "    moving_coord_grid: mrinr.typing.CoordGrid3D,\n",
        "    grid_fov: torch.Tensor,\n",
        "    grid_min_coord: torch.Tensor,\n",
        "    fixed_recon_iters: int,\n",
        "    moving_align_iters: int,\n",
        "    lambda_recon: float,\n",
        "    lambda_intermediate_sync: float,\n",
        "    inr_kwargs: dict,\n",
        "    optim_kwargs: dict,\n",
        "    fixed_lr_scheduler_kwargs: dict,\n",
        "    moving_lr_scheduler_kwargs: dict,\n",
        "    mi_kwargs: dict,\n",
        "    batch_size: int,\n",
        "    p_sample_coord_shift: float,\n",
        "    device,\n",
        "    rand_sampler_seed: int = 1,\n",
        "    optim_class=torch.optim.AdamW,\n",
        ") -> dict:\n",
        "    assert fixed.shape[1] == moving.shape[1] == 1\n",
        "\n",
        "    moving_spacing = mrinr.coords.spacing(moving_affine).flatten().cpu().numpy()\n",
        "    coord_patch_dataset = RandSingleVolCoordSampleDataset(\n",
        "        coord_grid=moving_coord_grid.squeeze(0).cpu(),\n",
        "        weight_mask=torch.ones(1, *moving_coord_grid.shape[1:-1]),\n",
        "        n_samples_per_batch=batch_size,\n",
        "        n_batches_per_epoch=fixed_recon_iters,\n",
        "        p_rand_coord_shift=p_sample_coord_shift,\n",
        "        rand_shift_ranges=(moving_spacing / 2).tolist(),\n",
        "        seed=rand_sampler_seed,\n",
        "    )\n",
        "    coord_patch_dataloader = monai.data.DataLoader(\n",
        "        coord_patch_dataset,\n",
        "        batch_size=None,\n",
        "        collate_fn=None,\n",
        "        num_workers=3,\n",
        "        prefetch_factor=3,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "    )\n",
        "    ######\n",
        "    # 1. Train the fixed INR on reconstruction alone.\n",
        "    # Create initially identical INRs for fixed and moving volumes.\n",
        "    fixed_inr = ImageReconINR(**inr_kwargs).to(device)\n",
        "    #!TESTING\n",
        "    moving_inr = ImageReconINR(**inr_kwargs).to(device)\n",
        "    # Copy weights from fixed INR to initialize.\n",
        "    #!\n",
        "    moving_inr.load_state_dict(fixed_inr.state_dict())\n",
        "    optim = optim_class(fixed_inr.parameters(), **optim_kwargs)\n",
        "\n",
        "    recon_loss = torch.nn.MSELoss().to(device)\n",
        "\n",
        "    optim.zero_grad()\n",
        "\n",
        "    print(\"Training fixed INR reconstruction only...\")\n",
        "    log_epochs = set(\n",
        "        [1]\n",
        "        + np.linspace(1, fixed_recon_iters + 1, num=20, dtype=int).tolist()\n",
        "        + [fixed_recon_iters]\n",
        "    )\n",
        "    fixed_inr_loss = dict(epoch=list(), network=list(), metric=list(), loss=list())\n",
        "\n",
        "    fixed_inr.train()\n",
        "    for epoch, coords in enumerate(coord_patch_dataloader, start=1):\n",
        "        coords = mrinr.nn.channels_as_coords(coords, has_batch_dim=True).to(device)\n",
        "        coords = einops.rearrange(coords, \"b x y z coord -> 1 (b x) y z coord\")\n",
        "        f = mrinr.grid_resample(\n",
        "            fixed.expand(coords.shape[0], -1, -1, -1, -1),\n",
        "            affine_x_el2coords=fixed_affine,\n",
        "            sample_coords=coords,\n",
        "            mode_or_interpolation=\"linear\",\n",
        "            padding_mode_or_bound=\"zeros\",\n",
        "            interp_lib=\"torch\",\n",
        "        )\n",
        "        pred_f = fixed_inr(\n",
        "            coords,\n",
        "            grid_fov=grid_fov,\n",
        "            grid_min_coord=grid_min_coord,\n",
        "            return_intermediates=False,\n",
        "        )\n",
        "        loss = recon_loss(pred_f, f)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        fixed_inr_loss[\"epoch\"].append(epoch)\n",
        "        fixed_inr_loss[\"network\"].append(\"fixed_inr\")\n",
        "        fixed_inr_loss[\"metric\"].append(\"recon\")\n",
        "        fixed_inr_loss[\"loss\"].append(loss.detach().item())\n",
        "\n",
        "        if epoch in log_epochs:\n",
        "            print(\n",
        "                f\"Epoch {epoch}/{fixed_recon_iters},\"\n",
        "                f\"Fixed recon loss: {loss.detach().item():.6f}, \"\n",
        "            )\n",
        "    optim.zero_grad()\n",
        "    print(\"Fixed INR pretraining complete.\")\n",
        "\n",
        "    # 2. Train the moving INR with both reconstruction and feature alignment losses.\n",
        "    coord_patch_dataset = RandSingleVolCoordSampleDataset(\n",
        "        coord_grid=moving_coord_grid.squeeze(0).cpu(),\n",
        "        weight_mask=torch.ones(1, *moving_coord_grid.shape[1:-1]),\n",
        "        n_samples_per_batch=batch_size,\n",
        "        n_batches_per_epoch=moving_align_iters,\n",
        "        p_rand_coord_shift=p_sample_coord_shift,\n",
        "        rand_shift_ranges=(moving_spacing / 2).tolist(),\n",
        "        seed=rand_sampler_seed,\n",
        "    )\n",
        "    coord_patch_dataloader = monai.data.DataLoader(\n",
        "        coord_patch_dataset,\n",
        "        batch_size=None,\n",
        "        collate_fn=None,\n",
        "        num_workers=3,\n",
        "        prefetch_factor=3,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    #! moving_inr = ImageReconINR(**inr_kwargs).to(device)\n",
        "    # # Copy weights from fixed INR to initialize.\n",
        "    #! moving_inr.load_state_dict(fixed_inr.state_dict())\n",
        "    optim = optim_class(moving_inr.parameters(), **optim_kwargs)\n",
        "    mi_loss = WeightedNMIParzenLoss(**mi_kwargs, reduction=\"none\").to(device)\n",
        "    fixed_inr.eval()\n",
        "    moving_inr.train()\n",
        "    moving_inr_loss = dict(epoch=list(), network=list(), metric=list(), loss=list())\n",
        "    print(\"Pretraining fixed and moving INRs...\")\n",
        "    log_epochs = set(\n",
        "        [1]\n",
        "        + np.linspace(1, moving_align_iters + 1, num=20, dtype=int).tolist()\n",
        "        + [moving_align_iters]\n",
        "    )\n",
        "    for epoch, coords in enumerate(coord_patch_dataloader, start=1):\n",
        "        coords = mrinr.nn.channels_as_coords(coords, has_batch_dim=True).to(device)\n",
        "        coords = einops.rearrange(coords, \"b x y z coord -> 1 (b x) y z coord\")\n",
        "\n",
        "        m = mrinr.grid_resample(\n",
        "            moving.expand(coords.shape[0], -1, -1, -1, -1),\n",
        "            affine_x_el2coords=moving_affine,\n",
        "            sample_coords=coords,\n",
        "            mode_or_interpolation=\"linear\",\n",
        "            padding_mode_or_bound=\"zeros\",\n",
        "            interp_lib=\"torch\",\n",
        "        )\n",
        "        pred_m, m_intermediates = moving_inr(\n",
        "            coords,\n",
        "            grid_fov=grid_fov,\n",
        "            grid_min_coord=grid_min_coord,\n",
        "            return_intermediates=True,\n",
        "        )\n",
        "\n",
        "        if lambda_intermediate_sync > 0.0:\n",
        "            with torch.no_grad():\n",
        "                _, f_intermediates = fixed_inr(\n",
        "                    coords,\n",
        "                    grid_fov=grid_fov,\n",
        "                    grid_min_coord=grid_min_coord,\n",
        "                    return_intermediates=True,\n",
        "                )\n",
        "            feature_align_mi = list()\n",
        "            for f_int, m_int in zip(f_intermediates, m_intermediates):\n",
        "                # Sum over the channel dimension, should be shape (B,)\n",
        "                feature_align_mi.append(\n",
        "                    mi_loss(\n",
        "                        f_int,\n",
        "                        m_int,\n",
        "                    ).mean(1)\n",
        "                )\n",
        "            feature_align_mi = torch.stack(feature_align_mi, dim=0).mean()\n",
        "        else:\n",
        "            feature_align_mi = torch.tensor(0.0).to(device)\n",
        "\n",
        "        loss_recon = recon_loss(pred_m, m)\n",
        "        loss = (lambda_recon * loss_recon) + (\n",
        "            lambda_intermediate_sync * feature_align_mi\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "        for metric_name, l in zip(\n",
        "            [\"recon\", \"feature_align_mi\"],\n",
        "            [\n",
        "                lambda_recon * loss_recon.detach().item(),\n",
        "                lambda_intermediate_sync * feature_align_mi.detach().item(),\n",
        "            ],\n",
        "        ):\n",
        "            moving_inr_loss[\"epoch\"].append(epoch)\n",
        "            moving_inr_loss[\"network\"].append(\"moving_inr\")\n",
        "            moving_inr_loss[\"metric\"].append(metric_name)\n",
        "            moving_inr_loss[\"loss\"].append(l)\n",
        "\n",
        "        if epoch in log_epochs:\n",
        "            print(\n",
        "                f\"Epoch {epoch}/{moving_align_iters},\"\n",
        "                f\"Moving recon loss: {lambda_recon * loss_recon.detach().item():.6f}, \"\n",
        "                f\"Intermediate MI loss: {lambda_intermediate_sync * feature_align_mi.detach().item():.6f}\"\n",
        "            )\n",
        "\n",
        "    optim.zero_grad()\n",
        "    print(\"Pretraining complete.\")\n",
        "\n",
        "    n_chunks = 16\n",
        "    moving_coord_grid_chunks = torch.chunk(moving_coord_grid, n_chunks, dim=-2)\n",
        "    pred_fixed_chunks = list()\n",
        "    pred_moving_chunks = list()\n",
        "    with torch.no_grad():\n",
        "        for moving_coord_grid_chunk in moving_coord_grid_chunks:\n",
        "            pred_f_chunk = fixed_inr(\n",
        "                moving_coord_grid_chunk,\n",
        "                grid_fov=grid_fov,\n",
        "                grid_min_coord=grid_min_coord,\n",
        "            )\n",
        "            pred_fixed_chunks.append(pred_f_chunk.cpu())\n",
        "            pred_m_chunk = moving_inr(\n",
        "                moving_coord_grid_chunk,\n",
        "                grid_fov=grid_fov,\n",
        "                grid_min_coord=grid_min_coord,\n",
        "            )\n",
        "            pred_moving_chunks.append(pred_m_chunk.cpu())\n",
        "    pred_fixed = torch.cat(pred_fixed_chunks, dim=-1)\n",
        "    pred_moving = torch.cat(pred_moving_chunks, dim=-1)\n",
        "\n",
        "    plt.clf()\n",
        "    plt.figure(dpi=250)\n",
        "    mrinr.viz.plot_1_ch_multi_slice_compare_vols(\n",
        "        fixed[0],\n",
        "        pred_fixed,\n",
        "        moving[0],\n",
        "        pred_moving,\n",
        "        vol_labels=[\n",
        "            \"Fixed Volume\",\n",
        "            \"Fixed INR Recon\",\n",
        "            \"Moving Volume\",\n",
        "            \"Moving INR Recon\",\n",
        "        ],\n",
        "        colorbars=\"global\",\n",
        "        axial_slice_idx=(0.35, 0.5),\n",
        "        saggital_slice_idx=0.45,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    return dict(\n",
        "        fixed_inr=fixed_inr,\n",
        "        moving_inr=moving_inr,\n",
        "        fixed_inr_loss=fixed_inr_loss,\n",
        "        moving_inr_loss=moving_inr_loss,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc36279b",
      "metadata": {},
      "outputs": [],
      "source": [
        "_dataset_dirs = {\n",
        "    \"mica_mics\": Path(\"~/mnt/magpie/outputs/mica_mics/derivatives/epinr_fmap_learning\")\n",
        "    .expanduser()\n",
        "    .resolve(),\n",
        "    \"vcu_ms\": Path(\"~/mnt/magpie/outputs/vcu_ms_epinr/derivatives/epinr_fmap_learning\")\n",
        "    .expanduser()\n",
        "    .resolve(),\n",
        "}\n",
        "_dataset_table_f = Path(\"data_tables/merged_dmri_dataset_table.csv\").resolve()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c139151f",
      "metadata": {},
      "source": [
        "## Susceptibility Distortion Field Reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06391eaa",
      "metadata": {},
      "source": [
        "### Whole-Volume Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae887da1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment params\n",
        "params = Box(\n",
        "    dict(\n",
        "        dataset_dirs=_dataset_dirs,\n",
        "        dataset_table_f=_dataset_table_f,\n",
        "        result_base_dir=Path(\"~/mnt/magpie/outputs/results/epinr/dmri/epinr/\")\n",
        "        .expanduser()\n",
        "        .resolve(),\n",
        "        # epochs=75,\n",
        "        # n_repeats_per_epoch=40,\n",
        "        epochs=5,\n",
        "        n_repeats_per_epoch=5,\n",
        "        effective_batch_size=8,\n",
        "        batch_size=2,\n",
        "        patch_size=(32, 32, 32),\n",
        "        winsorize_quantiles=(0.01, 0.99),\n",
        "        feature_range=(0.0, 1.0),\n",
        "        mask_blur_sigma_mm=2.0,\n",
        "        mask_blur_truncate=5.0,\n",
        "        # INR model params\n",
        "        hidden_features=256,\n",
        "        num_layers=5,\n",
        "        omega=1.0,\n",
        "        omega_0=30.0,\n",
        "        c=6.0,\n",
        "        # Learnable affine transform params.\n",
        "        # Moving volume non-rigid affine components.\n",
        "        non_rigid_affine_layer=False,\n",
        "        non_rigid_affine_nonzero_thresh=None,\n",
        "        # Fixed volume, refined with rigid affine only.\n",
        "        refine_fixed_rigid_affine=True,\n",
        "        # Percentage of training epochs that will optimize the fixed volume alignment.\n",
        "        # After this percentage of epochs, the fixed volume rigid affine will be\n",
        "        # frozen.\n",
        "        refine_fixed_rigid_affine_epoch_percent=0.5,\n",
        "        # Positional encoding params\n",
        "        m_num_freqs=None,\n",
        "        pos_enc_sigma=None,\n",
        "        pos_enc_type=None,\n",
        "        # m_num_freqs=512,\n",
        "        # pos_enc_sigma=0.5,\n",
        "        # pos_enc_type=\"gaussian\",\n",
        "        # Training function params\n",
        "        # Pretrain to warped atlas.\n",
        "        pretrain_to_atlas=False,\n",
        "        pretrain_epochs=None,\n",
        "        optim_lr=0.0005,\n",
        "        optim_weight_decay=0.01,\n",
        "        optim_betas=(0.9, 0.999),\n",
        "        # Linear LR scheduler end scale factor.\n",
        "        lr_scheduler_end_factor=0.01,\n",
        "        # Percentage of training epochs for constant warmup and constant cooldown\n",
        "        # phases, both before and after the linear decay phase. Each warmup and cooldown\n",
        "        # phase lasts for this percentage, so the linear decay phase lasts for\n",
        "        # 1 - (2 * this percentage).\n",
        "        lr_scheduler_warmup_cooldown_percent=0.15,\n",
        "        enable_jac_mod_in_training=True,\n",
        "        mse_sim_weight=0.0,\n",
        "        mi_sim_weight=1.0,\n",
        "        mi_sim_kwargs=dict(\n",
        "            num_bins=32, sigma_ratio=0.5, norm_mi=False, norm_images=True\n",
        "        ),\n",
        "        ncc_sim_weight=0.0,\n",
        "        lncc_sim_weight=0.0,\n",
        "        lncc_sim_kwargs=dict(kernel_size=9, kernel_type=\"rectangular\"),\n",
        "        laplacian_ncc_sim_weight=0.5,\n",
        "        laplacian_mse_sim_weight=0.0,\n",
        "        laplacian_sigma_low=0.5,\n",
        "        laplacian_truncate=2.0,\n",
        "        jac_fro_reg_weight=1.5,\n",
        "        bending_energy_reg_weight=2.0,\n",
        "        resample_kwargs=dict(\n",
        "            mode_or_interpolation=\"linear\",\n",
        "            padding_mode_or_bound=\"zeros\",\n",
        "            interp_lib=\"torch\",\n",
        "        ),\n",
        "        val_mi_sim_kwargs=dict(kernel_type=\"gaussian\", num_bins=32, sigma_ratio=0.5),\n",
        "    ),\n",
        ")\n",
        "params.result_base_dir.mkdir(parents=True, exist_ok=True)\n",
        "grad_accumulate_steps = params.effective_batch_size / params.batch_size\n",
        "assert grad_accumulate_steps.is_integer(), (\n",
        "    f\"Effective batch size {params.effective_batch_size} must be \"\n",
        "    f\"a multiple of batch size {params.batch_size}.\"\n",
        ")\n",
        "grad_accumulate_steps = int(grad_accumulate_steps)\n",
        "n_batches_per_epoch = params.n_repeats_per_epoch * grad_accumulate_steps\n",
        "viz_validation_every: int = max(params.epochs // 10, 1)  # epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5181113",
      "metadata": {},
      "outputs": [],
      "source": [
        "# #!\n",
        "# pretrain_results = pretrain_sequential_fixed_moving_inrs(\n",
        "#     fixed=subj_data.t1w,\n",
        "#     fixed_affine=subj_data.t1w_affine,\n",
        "#     moving=subj_data.b0,\n",
        "#     moving_affine=subj_data.b0_affine,\n",
        "#     moving_coord_grid=subj_data.b0_scanner_coord_grid,\n",
        "#     grid_fov=subj_data.b0_fov,\n",
        "#     grid_min_coord=subj_data.b0_min_coord,\n",
        "#     fixed_recon_iters=100_000,\n",
        "#     moving_align_iters=100_000,\n",
        "#     lambda_recon=1.0,\n",
        "#     lambda_intermediate_sync=0.0,\n",
        "#     inr_kwargs=dict(\n",
        "#         hidden_features=256,\n",
        "#         num_layers=5,\n",
        "#         out_features=1,\n",
        "#         m_num_freqs=1024,\n",
        "#         sigma=5.0,\n",
        "#         pos_enc=\"gaussian\",\n",
        "#     ),\n",
        "#     optim_kwargs=dict(\n",
        "#         lr=0.00001,\n",
        "#         weight_decay=params.optim_weight_decay,\n",
        "#         betas=params.optim_betas,\n",
        "#     ),\n",
        "#     mi_kwargs=dict(num_bins=16, sigma_ratio=0.5, norm_mi=True, norm_images=False),\n",
        "#     batch_size=20_000,\n",
        "#     p_sample_coord_shift=0.5,\n",
        "#     device=device,\n",
        "# )\n",
        "# #!\n",
        "# pretrain_fixed_recon_losses = pd.DataFrame(pretrain_results[\"fixed_inr_loss\"])\n",
        "# pretrain_moving_losses = pd.DataFrame(pretrain_results[\"moving_inr_loss\"])\n",
        "# pretrain_losses = pd.concat(\n",
        "#     [pretrain_fixed_recon_losses, pretrain_moving_losses], axis=0\n",
        "# )\n",
        "# plt.clf()\n",
        "# plt.figure(dpi=150)\n",
        "# fg = sns.relplot(\n",
        "#     data=pretrain_losses,\n",
        "#     x=\"epoch\",\n",
        "#     y=\"loss\",\n",
        "#     hue=\"network\",\n",
        "#     col=\"metric\",\n",
        "#     kind=\"line\",\n",
        "#     alpha=0.8,\n",
        "#     facet_kws={\"sharey\": False},\n",
        "# )\n",
        "# fg.axes[0, 0].set_yscale(\"log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "689f2a37",
      "metadata": {},
      "outputs": [],
      "source": [
        "def validation_step(\n",
        "    epinr: EPINR,\n",
        "    subj_data: DWISubjectData,\n",
        "    params: Box,\n",
        "    epoch: int,\n",
        "    step: int,\n",
        "    val_score_fns: dict,\n",
        "    b0_spacing: torch.Tensor,\n",
        "    viz_validation_every: int,\n",
        "    pr=print,\n",
        ") -> dict:\n",
        "    # Convenience functions for resampling volumes.\n",
        "    sample_t1w = lambda c: mrinr.grid_resample(\n",
        "        subj_data.t1w.expand(c.shape[0], -1, -1, -1, -1),\n",
        "        affine_x_el2coords=subj_data.t1w_affine,\n",
        "        sample_coords=c,\n",
        "        **params.resample_kwargs,\n",
        "    )\n",
        "    sample_b0 = lambda c: mrinr.grid_resample(\n",
        "        subj_data.b0.expand(c.shape[0], -1, -1, -1, -1),\n",
        "        affine_x_el2coords=subj_data.b0_affine,\n",
        "        sample_coords=c,\n",
        "        **params.resample_kwargs,\n",
        "    )\n",
        "\n",
        "    # Break up the prediction into smaller chunks to avoid OOM errors.\n",
        "    n_chunks = 16\n",
        "    b0_coord_grid_chunks = torch.chunk(\n",
        "        subj_data.b0_scanner_coord_grid, n_chunks, dim=-2\n",
        "    )\n",
        "    pred_displacement_field_mm_chunks = list()\n",
        "    J_chunks = list()\n",
        "    J_deform_chunks = list()\n",
        "    for b0_coord_grid_chunk in b0_coord_grid_chunks:\n",
        "        pred_displacement_field_mm_chunk = epinr(\n",
        "            x=b0_coord_grid_chunk,\n",
        "            grid_fov=subj_data.b0_fov,\n",
        "            grid_min_coord=subj_data.b0_min_coord,\n",
        "        )\n",
        "        pred_displacement_field_mm_chunks.append(pred_displacement_field_mm_chunk)\n",
        "        J_chunk = jacobian_matrix_batched(\n",
        "            pred_displacement_net=epinr,\n",
        "            x_coords=b0_coord_grid_chunk,\n",
        "            grid_fov=subj_data.b0_fov,\n",
        "            grid_min_coord=subj_data.b0_min_coord,\n",
        "            jac_of_warp_type=\"displacement\",\n",
        "        )\n",
        "        J_chunks.append(J_chunk)\n",
        "        J_deform_chunk = jacobian_matrix_batched(\n",
        "            pred_displacement_net=epinr,\n",
        "            x_coords=b0_coord_grid_chunk,\n",
        "            grid_fov=subj_data.b0_fov,\n",
        "            grid_min_coord=subj_data.b0_min_coord,\n",
        "            jac_of_warp_type=\"deformation\",\n",
        "        )\n",
        "        J_deform_chunks.append(J_deform_chunk)\n",
        "    pred_displacement_field_mm = torch.cat(pred_displacement_field_mm_chunks, dim=-2)\n",
        "    J = torch.cat(J_chunks, dim=-3)\n",
        "    det_J_ap = 1 + J[:, None, ..., 1, 1]\n",
        "    J_deform = torch.cat(J_deform_chunks, dim=-3)\n",
        "    # Sample the moving b0 volume with the predicted displacement field.\n",
        "    moving_b0_sample = det_J_ap * sample_b0(\n",
        "        subj_data.b0_scanner_coord_grid + pred_displacement_field_mm\n",
        "    )\n",
        "\n",
        "    ## Compute validation scores.\n",
        "    # Sample the fixed volume without any affine refinement.\n",
        "    t1w_coord_grid = mrinr.coords.affine_coord_grid(\n",
        "        subj_data.t1w_affine,\n",
        "        subj_data.t1w.shape[-3:],\n",
        "    ).to(subj_data.b0_scanner_coord_grid)\n",
        "    # Sample T1w at the coordinates of the moving image for quant. comparison.\n",
        "    fixed_t1w_sample = sample_t1w(subj_data.b0_scanner_coord_grid)\n",
        "    # Also sample at the full T1w resolution for visualization.\n",
        "    viz_fixed_t1w_sample = sample_t1w(t1w_coord_grid)\n",
        "    if epinr.fixed2moving_rigid_affine is not None:\n",
        "        rigid_refine_fixed_t1w_sample = sample_t1w(\n",
        "            epinr.apply_fixed_rigid_affine(t1w_coord_grid)\n",
        "        )\n",
        "    else:\n",
        "        rigid_refine_fixed_t1w_sample = None\n",
        "    # Convert the topup ground truth displacement field to mm units.\n",
        "    topup_suscept_field_mm = (\n",
        "        subj_data.topup_displace_hz\n",
        "        * subj_data.total_readout_time_s\n",
        "        * b0_spacing[0, 1]\n",
        "        * (-1 if subj_data.pe_dir == \"ap\" else 1)\n",
        "    )[:, 0]\n",
        "    val_scores_epoch_i = {\n",
        "        \"fixed_moving_mi\": val_score_fns[\"fixed_moving_mi\"](\n",
        "            moving_b0_sample, fixed_t1w_sample\n",
        "        )\n",
        "        .cpu()\n",
        "        .item(),\n",
        "        \"fixed_moving_ncc\": val_score_fns[\"fixed_moving_ncc\"](\n",
        "            moving_b0_sample, fixed_t1w_sample\n",
        "        )\n",
        "        .cpu()\n",
        "        .item(),\n",
        "        \"topup_suscept_mse\": val_score_fns[\"topup_suscept_mse\"](\n",
        "            pred_displacement_field_mm[..., 1], topup_suscept_field_mm\n",
        "        )\n",
        "        .cpu()\n",
        "        .item(),\n",
        "        \"topup_corrected_b0_mse\": val_score_fns[\"topup_corrected_b0_mse\"](\n",
        "            moving_b0_sample, subj_data.topup_corrected_b0\n",
        "        )\n",
        "        .cpu()\n",
        "        .item(),\n",
        "        \"masked_topup_suscept_mse\": val_score_fns[\"masked_topup_suscept_mse\"](\n",
        "            pred_displacement_field_mm[..., 1],\n",
        "            topup_suscept_field_mm,\n",
        "            subj_data.b0_mask,\n",
        "        )\n",
        "        .cpu()\n",
        "        .item(),\n",
        "        \"masked_topup_corrected_b0_mse\": val_score_fns[\"masked_topup_corrected_b0_mse\"](\n",
        "            moving_b0_sample, subj_data.topup_corrected_b0, subj_data.b0_mask\n",
        "        )\n",
        "        .cpu()\n",
        "        .item(),\n",
        "        \"det_J_def_neg_frac\": val_score_fns[\"det_J_def_neg_frac\"](J_deform)\n",
        "        .cpu()\n",
        "        .item(),\n",
        "    }\n",
        "    pr(f\"Validation scores at epoch {epoch}:\")\n",
        "    for metric_name, val_score in val_scores_epoch_i.items():\n",
        "        # val_scores[\"epoch\"].append(epoch)\n",
        "        # val_scores[\"step\"].append(step)\n",
        "        # val_scores[\"metric\"].append(metric_name)\n",
        "        # val_scores[\"val\"].append(val_score)\n",
        "        pr(f\"{metric_name}: {val_score: 10.9f}\", end=\" | \")\n",
        "    pr(flush=True)\n",
        "\n",
        "    if epinr.nonrigid_affine_layer is not None:\n",
        "        pr(\n",
        "            \"Nonrigid affine for moving image\\n\",\n",
        "            {\n",
        "                k: v.detach().cpu().numpy()\n",
        "                for k, v in epinr.nonrigid_affine_layer.named_parameters()\n",
        "            },\n",
        "        )\n",
        "    if epinr.fixed_rigid_affine is not None:\n",
        "        pr(\n",
        "            \"Rigid affine for fixed image refinement:\\n\",\n",
        "            {\n",
        "                k: v.detach().cpu().numpy()\n",
        "                for k, v in epinr.fixed_rigid_affine.named_parameters()\n",
        "            },\n",
        "        )\n",
        "\n",
        "    if viz_validation_every and (\n",
        "        epoch % viz_validation_every == 0 or epoch == params.epochs\n",
        "    ):\n",
        "        plt.clf()\n",
        "        plt.figure(dpi=175)\n",
        "        vols = list()\n",
        "        vol_labels = list()\n",
        "        vols.extend([subj_data.b0, moving_b0_sample, viz_fixed_t1w_sample])\n",
        "        vol_labels.extend([\"In Dist b0\", \"Pred b0 Undist.\", \"T1\"])\n",
        "        if rigid_refine_fixed_t1w_sample is not None:\n",
        "            vols.append(rigid_refine_fixed_t1w_sample)\n",
        "            vol_labels.append(\"Refine T1\")\n",
        "        vols.extend(\n",
        "            [\n",
        "                torch.abs(subj_data.b0 - moving_b0_sample),\n",
        "                subj_data.topup_corrected_b0,\n",
        "                torch.abs(subj_data.topup_corrected_b0 - moving_b0_sample),\n",
        "                subj_data.topup_displace_hz\n",
        "                * subj_data.total_readout_time_s\n",
        "                * b0_spacing[0, 1]\n",
        "                * (-1 if subj_data.pe_dir == \"ap\" else 1),\n",
        "                pred_displacement_field_mm[..., 1],\n",
        "                det_J_ap,\n",
        "            ]\n",
        "        )\n",
        "        vol_labels.extend(\n",
        "            [\n",
        "                \"b0 Dist - Undist\",\n",
        "                \"Topup b0\",\n",
        "                \"Topup - Pred\",\n",
        "                \"Topup Disp mm Y\",\n",
        "                \"Pred Disp mm Y\",\n",
        "                \"Det J\",\n",
        "            ]\n",
        "        )\n",
        "        mrinr.viz.plot_1_ch_multi_slice_compare_vols(\n",
        "            *vols,\n",
        "            colorbars=\"rows\",\n",
        "            vol_labels=vol_labels,\n",
        "            axial_slice_idx=(0.35, 0.5),\n",
        "            saggital_slice_idx=0.45,\n",
        "        )\n",
        "        if epoch != params.epochs:\n",
        "            plt.show()\n",
        "\n",
        "    return {\n",
        "        \"val_scores\": val_scores_epoch_i,\n",
        "        \"pred_displacement_field_mm\": pred_displacement_field_mm[0, ..., 1]\n",
        "        .cpu()\n",
        "        .numpy(),\n",
        "        \"pred_undistorted_b0\": moving_b0_sample[0, 0].cpu().numpy(),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fd42b5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------------\n",
        "FULL_EXPERIMENT = True\n",
        "\n",
        "#### Training loop.\n",
        "dataset_table = pd.read_csv(params.dataset_table_f, index_col=None, comment=\"#\")\n",
        "if FULL_EXPERIMENT:\n",
        "    dataset_table_rows = list(range(len(dataset_table)))\n",
        "    # Shuffle subject order to get better estimate of generalization performance, earlier.\n",
        "    dataset_table_rows = np.random.shuffle(list(dataset_table_rows))\n",
        "    # dataset_table_rows = [45, 56]  #!\n",
        "    timestamp_str = mrinr.utils.timestamp_now()\n",
        "    experiment_label = \"epinr_dmri\"\n",
        "    # New experiment name.\n",
        "    experiment_name = f\"{timestamp_str}_{experiment_label}\"\n",
        "    # Fixed experiment name.\n",
        "    # experiment_name = \"\"\n",
        "    final_result_dir = params.result_base_dir / experiment_name\n",
        "    experiment_result_dir = final_result_dir\n",
        "    # Fixed result dir.\n",
        "    # tmp_result_dir_name = final_result_dir.parent / \"\"\n",
        "    # Random tmp result dir.\n",
        "    tmp_result_dir_name = tempfile.mkdtemp(\n",
        "        dir=final_result_dir.parent,\n",
        "        prefix=f\"tmp-{timestamp_str}-\",\n",
        "        suffix=f\"__{final_result_dir.name}\",\n",
        "    )\n",
        "    tmp_result_dir = Path(tmp_result_dir_name)\n",
        "    result_dir = tmp_result_dir\n",
        "    # Save out parameters used for the experiment.\n",
        "    p = copy.deepcopy(params.to_dict())\n",
        "    with open(result_dir / \"experiment_params.yaml\", \"w\") as f:\n",
        "        yaml.dump(params.to_dict(), f)\n",
        "else:\n",
        "    dataset_table_rows = [45]\n",
        "    result_dir = Path(\"tmp\").resolve()\n",
        "    final_result_dir = None\n",
        "    experiment_result_dir = None\n",
        "\n",
        "for dataset_table_row in dataset_table_rows:\n",
        "    subj_table = dataset_table.iloc[dataset_table_row : (dataset_table_row + 1)]\n",
        "    print(f\"Starting subject {subj_table['subj_id'].values.item()}...\")\n",
        "\n",
        "    # Handle random seeding.\n",
        "    if FULL_EXPERIMENT:\n",
        "        random_seed = mrinr.utils.create_subj_rng_seed(\n",
        "            base_rng_seed=1, subj_id=subj_table[\"subj_id\"].values.item()\n",
        "        ) % (2**32 - 1)\n",
        "    else:\n",
        "        random_seed = np.random.randint(0, 2**32 - 1)\n",
        "        # random_seed = 4221131316\n",
        "    torch.random.manual_seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "    patch_select_seed = np.random.randint(0, 2**32 - 1)\n",
        "\n",
        "    subj_data = load_dwi_subject_data(\n",
        "        dataset_table=subj_table,\n",
        "        dataset_dirs=params.dataset_dirs,\n",
        "        device=device,\n",
        "    )[0]\n",
        "\n",
        "    # Create a subject-specific result directory.\n",
        "    if FULL_EXPERIMENT:\n",
        "        subj_result_dir = (\n",
        "            result_dir\n",
        "            / f\"{subj_data.dataset_name}_{subj_data.subj_id}_dir-{subj_data.pe_dir}\"\n",
        "        )\n",
        "        subj_result_dir.mkdir(parents=True, exist_ok=True)\n",
        "        completed_indicator_f = subj_result_dir / \"epinr_final_model.pt\"\n",
        "        if completed_indicator_f.exists():\n",
        "            print(\n",
        "                f\"Subject {subj_data.subj_id} already completed,\"\n",
        "                \" skipping to next subject...\"\n",
        "            )\n",
        "            continue\n",
        "        log_f = subj_result_dir / \"log.txt\"\n",
        "        if log_f.exists():\n",
        "            log_mtime_delta = time.time() - os.path.getmtime(log_f)\n",
        "            if log_mtime_delta < 30.0:\n",
        "                print(\n",
        "                    f\"Subject {subj_data.subj_id} is currently running,\"\n",
        "                    \" skipping to next subject...\"\n",
        "                )\n",
        "                continue\n",
        "            log_f.unlink(missing_ok=True)\n",
        "        pr = partial(mrinr.utils.tee, file=log_f)\n",
        "        pr(f\"Subject row:\\n{dataset_table_row}\")\n",
        "    else:\n",
        "        subj_result_dir = result_dir\n",
        "        pr = print\n",
        "\n",
        "    pr(f\"Random seed: {random_seed}\")\n",
        "    pr(f\"Loaded subject {subj_data.dataset_name}, {subj_data.subj_id}.\")\n",
        "    # Store predicted displacement fields and undistorted b0 volumes at each validation\n",
        "    # step.\n",
        "    val_pred_displacement_fields_f = (\n",
        "        subj_result_dir / \"val_pred_displacement_fields.nii.gz\"\n",
        "    )\n",
        "    val_undistorted_b0s_f = subj_result_dir / \"val_undistorted_b0s.nii.gz\"\n",
        "    final_val_fig_f = subj_result_dir / \"final_validation_result.png\"\n",
        "    train_scores_f = subj_result_dir / \"train_scores.csv\"\n",
        "    val_scores_f = subj_result_dir / \"val_scores.csv\"\n",
        "    train_loss_plot_f = subj_result_dir / \"train_loss_plot.png\"\n",
        "    val_scores_plot_f = subj_result_dir / \"val_loss_plot.png\"\n",
        "    val_pred_displacement_fields_list = list()\n",
        "    val_pred_undistorted_b0s_list = list()\n",
        "\n",
        "    # Prep volumes for training.\n",
        "    train_vols = prep_vols(\n",
        "        b0=subj_data.b0,\n",
        "        b0_mask=subj_data.b0_mask,\n",
        "        b0_affine=subj_data.b0_affine,\n",
        "        t1w=subj_data.t1w,\n",
        "        t1w_mask=subj_data.t1w_mask,\n",
        "        t1w_affine=subj_data.t1w_affine,\n",
        "        scale_kwargs=dict(\n",
        "            winsorize_quantiles=params.winsorize_quantiles,\n",
        "            feature_range=params.feature_range,\n",
        "        ),\n",
        "        blur_mask_kwargs=dict(\n",
        "            sigma_mm=params.mask_blur_sigma_mm,\n",
        "            truncate=params.mask_blur_truncate,\n",
        "        ),\n",
        "    )\n",
        "    subj_data.b0 = train_vols[\"b0\"]\n",
        "    subj_data.t1w = train_vols[\"t1w\"]\n",
        "    b0_weight_mask = train_vols[\"b0_mask\"]\n",
        "    t1w_weight_mask = train_vols[\"t1w_mask\"]\n",
        "    subj_data.topup_corrected_b0 = scale_vol(\n",
        "        subj_data.topup_corrected_b0,\n",
        "        winsorize_quantiles=params.winsorize_quantiles,\n",
        "        feature_range=params.feature_range,\n",
        "    )\n",
        "\n",
        "    # Append a batch dimension to all Tensors.\n",
        "    data_d = dict()\n",
        "    for k, v in dataclasses.asdict(subj_data).items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            if \"fov\" in k:\n",
        "                v = einops.rearrange(v, \"ndim -> 1 1 1 ndim\")\n",
        "            elif \"min_coord\" in k:\n",
        "                v = einops.rearrange(v, \"ndim -> 1 1 1 ndim\")\n",
        "            data_d[k] = v.unsqueeze(0)\n",
        "        else:\n",
        "            data_d[k] = v\n",
        "    subj_data = DWISubjectData(**data_d)\n",
        "    b0_spacing = mrinr.coords.spacing(subj_data.b0_affine)\n",
        "    b0_weight_mask.unsqueeze_(0)\n",
        "    t1w_weight_mask.unsqueeze_(0)\n",
        "    # 1 / 0\n",
        "\n",
        "    # Convenience functions for resampling volumes.\n",
        "    sample_t1w = lambda c: mrinr.grid_resample(\n",
        "        subj_data.t1w.expand(c.shape[0], -1, -1, -1, -1),\n",
        "        affine_x_el2coords=subj_data.t1w_affine,\n",
        "        sample_coords=c,\n",
        "        **params.resample_kwargs,\n",
        "    )\n",
        "    sample_b0 = lambda c: mrinr.grid_resample(\n",
        "        subj_data.b0.expand(c.shape[0], -1, -1, -1, -1),\n",
        "        affine_x_el2coords=subj_data.b0_affine,\n",
        "        sample_coords=c,\n",
        "        **params.resample_kwargs,\n",
        "    )\n",
        "    sample_t1w_weight_mask = lambda c: mrinr.grid_resample(\n",
        "        t1w_weight_mask.expand(c.shape[0], -1, -1, -1, -1),\n",
        "        affine_x_el2coords=subj_data.t1w_affine,\n",
        "        sample_coords=c,\n",
        "        **params.resample_kwargs,\n",
        "    )\n",
        "    sample_b0_weight_mask = lambda c: mrinr.grid_resample(\n",
        "        b0_weight_mask.expand(c.shape[0], -1, -1, -1, -1),\n",
        "        affine_x_el2coords=subj_data.b0_affine,\n",
        "        sample_coords=c,\n",
        "        **params.resample_kwargs,\n",
        "    )\n",
        "\n",
        "    # Construct a dataset and dataloader to split up the coordinate grids into patches, to\n",
        "    # save memory during training.\n",
        "    # The dataset needs a non-batched volume with channel dimensions (not coordinate\n",
        "    # dimensions) on the cpu. So, move the coordinate grid to the cpu, remove the batch\n",
        "    # dim, and convert the coord dim to a channel dim.\n",
        "    to_be_patched_coord_grid = mrinr.nn.coords_as_channels(\n",
        "        subj_data.b0_scanner_coord_grid.squeeze(0).cpu(), has_batch_dim=False\n",
        "    )\n",
        "    # Minibatch of smaller patches that are randomly drawn from the coordinate grid.\n",
        "    # Combine the b0 and t1w weight masks for sampling.\n",
        "    combined_b0_t1w_weight_mask = torch.maximum(\n",
        "        sample_b0_weight_mask(subj_data.b0_scanner_coord_grid),\n",
        "        sample_t1w_weight_mask(subj_data.b0_scanner_coord_grid),\n",
        "    )\n",
        "    coord_patch_dataset = monai.data.PatchDataset(\n",
        "        # Data is a shallow copy of the same input coordinate grid, one copy for each batch\n",
        "        # in the epoch.\n",
        "        data=[\n",
        "            {\n",
        "                \"coords\": to_be_patched_coord_grid,\n",
        "                \"weight_mask\": combined_b0_t1w_weight_mask.squeeze(0).cpu(),\n",
        "            }\n",
        "        ]\n",
        "        * n_batches_per_epoch,\n",
        "        patch_func=monai.transforms.RandWeightedCropDict(\n",
        "            keys=\"coords\",\n",
        "            w_key=\"weight_mask\",\n",
        "            spatial_size=params.patch_size,\n",
        "            num_samples=params.batch_size,\n",
        "        ).set_random_state(seed=patch_select_seed),\n",
        "        samples_per_image=params.batch_size,\n",
        "    )\n",
        "    ######\n",
        "    coord_patch_dataloader = monai.data.DataLoader(\n",
        "        coord_patch_dataset,\n",
        "        batch_size=params.batch_size,\n",
        "        num_workers=4,\n",
        "        prefetch_factor=3,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    # Instantiate the EPINR model.\n",
        "    epinr = EPINR(\n",
        "        hidden_features=params.hidden_features,\n",
        "        num_layers=params.num_layers,\n",
        "        pe_dir=subj_data.pe_dir,\n",
        "        non_rigid_affine=params.non_rigid_affine_layer,\n",
        "        non_rigid_affine_nonzero_thresh=params.non_rigid_affine_nonzero_thresh,\n",
        "        sigma=params.pos_enc_sigma,\n",
        "        m_num_freqs=params.m_num_freqs,\n",
        "        pos_enc=params.pos_enc_type,\n",
        "        refine_fixed_rigid_affine=params.refine_fixed_rigid_affine,\n",
        "        omega=params.omega,\n",
        "        omega_0=params.omega_0,\n",
        "        c=params.c,\n",
        "    )\n",
        "    epinr.to(device=device)\n",
        "    epinr.train()\n",
        "    pr(\"Model summary:\")\n",
        "    pr(epinr)\n",
        "    # Instantiate the optimizer.\n",
        "    optim = torch.optim.AdamW(\n",
        "        [{\"params\": epinr.network_parameters()}]\n",
        "        + [{\"params\": epinr.fixed2moving_rigid_affine_parameters()}]\n",
        "        if params.refine_fixed_rigid_affine\n",
        "        else list(),\n",
        "        lr=params.optim_lr / grad_accumulate_steps,\n",
        "        betas=params.optim_betas,\n",
        "        weight_decay=params.optim_weight_decay,\n",
        "        fused=True,\n",
        "    )\n",
        "    pr(\"Optimizer:\")\n",
        "    pr(optim)\n",
        "    optim.zero_grad()\n",
        "    warm_cooldown_epochs = math.ceil(\n",
        "        params.epochs * params.lr_scheduler_warmup_cooldown_percent\n",
        "    )\n",
        "    lin_decay_lr_epochs = params.epochs - (2 * warm_cooldown_epochs)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
        "        optim,\n",
        "        [\n",
        "            torch.optim.lr_scheduler.ConstantLR(\n",
        "                optim, factor=1.0, total_iters=warm_cooldown_epochs\n",
        "            ),\n",
        "            torch.optim.lr_scheduler.LinearLR(\n",
        "                optim,\n",
        "                start_factor=1.0,\n",
        "                end_factor=params.lr_scheduler_end_factor,\n",
        "                total_iters=lin_decay_lr_epochs,\n",
        "            ),\n",
        "            torch.optim.lr_scheduler.ConstantLR(\n",
        "                optim,\n",
        "                factor=params.lr_scheduler_end_factor,\n",
        "                total_iters=warm_cooldown_epochs + 1,\n",
        "            ),\n",
        "        ],\n",
        "        milestones=[warm_cooldown_epochs, lin_decay_lr_epochs + warm_cooldown_epochs],\n",
        "    )\n",
        "    pr(\"LR Scheduler milestones:\")\n",
        "    pr(lr_scheduler._milestones)\n",
        "    if params.refine_fixed_rigid_affine:\n",
        "        stop_rigid_aff_epoch = math.ceil(\n",
        "            params.refine_fixed_rigid_affine_epoch_percent * params.epochs\n",
        "        )\n",
        "        pr(f\"Will stop refining fixed rigid affine at epoch {stop_rigid_aff_epoch}.\")\n",
        "    else:\n",
        "        stop_rigid_aff_epoch = math.inf\n",
        "\n",
        "    # Loss functions.\n",
        "    mse_loss = torch.nn.MSELoss(reduction=\"none\").to(device)\n",
        "    mi_loss = WeightedNMIParzenLoss(**params.mi_sim_kwargs, reduction=\"mean\").to(device)\n",
        "    ncc_loss = NCC(use_mask=False).to(device)\n",
        "    lncc_loss = monai.losses.LocalNormalizedCrossCorrelationLoss(\n",
        "        spatial_dims=3, **params.lncc_sim_kwargs, reduction=\"mean\"\n",
        "    ).to(device)\n",
        "    laplacian_fn = DoGLaplacian(\n",
        "        sigma_low=params.laplacian_sigma_low, truncate=params.laplacian_truncate\n",
        "    ).to(device)\n",
        "    loss_fn = mrinr.nn.loss.WeightedSumLoss(\n",
        "        loss_fns={\n",
        "            \"weighted_mse_sim\": lambda pred, target, weight_mask: (\n",
        "                mse_loss(pred, target) * weight_mask\n",
        "            ).mean(),\n",
        "            \"weighted_mi_sim\": mi_loss,\n",
        "            \"weighted_ncc_sim\": ncc_loss,\n",
        "            \"weighted_lncc_sim\": lncc_loss,\n",
        "            \"laplace_ncc_sim\": lambda pred, target: lncc_loss(\n",
        "                laplacian_fn(pred),\n",
        "                laplacian_fn(1 - target),  # 1-T1w to invert contrast\n",
        "                # laplacian_fn(target),  #! do not invert contrast\n",
        "            ),\n",
        "            \"laplace_mse_sim\": lambda pred, target, weight_mask: (\n",
        "                mse_loss(\n",
        "                    laplacian_fn(pred),\n",
        "                    laplacian_fn(1 - target),  # 1-T1w to invert contrast\n",
        "                    # laplacian_fn(target),  #! do not invert contrast\n",
        "                )\n",
        "                * weight_mask\n",
        "            ).mean(),\n",
        "            \"jac_fro_regularization\": jac_displace_fro_regularization,\n",
        "            \"bending_energy_regularization\": lambda model,\n",
        "            c: bending_energy_regularization(\n",
        "                model,\n",
        "                x_coords=c,\n",
        "                grid_fov=subj_data.b0_fov,\n",
        "                grid_min_coord=subj_data.b0_min_coord,\n",
        "                pe_dir=subj_data.pe_dir,\n",
        "                hessian_of_warp_type=\"deformation\",\n",
        "                compile_hessian=True,\n",
        "            ),\n",
        "        },\n",
        "        loss_weights=[\n",
        "            params.mse_sim_weight,\n",
        "            params.mi_sim_weight,\n",
        "            params.ncc_sim_weight,\n",
        "            params.lncc_sim_weight,\n",
        "            params.laplacian_ncc_sim_weight,\n",
        "            params.laplacian_mse_sim_weight,\n",
        "            params.jac_fro_reg_weight,\n",
        "            params.bending_energy_reg_weight,\n",
        "        ],\n",
        "    ).to(device)\n",
        "    val_score_fns = {\n",
        "        \"fixed_moving_mi\": monai.losses.GlobalMutualInformationLoss(\n",
        "            **params.val_mi_sim_kwargs, reduction=\"mean\"\n",
        "        ).to(device),\n",
        "        \"fixed_moving_ncc\": monai.losses.LocalNormalizedCrossCorrelationLoss(\n",
        "            spatial_dims=3, **params.lncc_sim_kwargs, reduction=\"mean\"\n",
        "        ).to(device),\n",
        "        \"topup_suscept_mse\": torch.nn.MSELoss(reduction=\"mean\").to(device),\n",
        "        \"topup_corrected_b0_mse\": torch.nn.MSELoss(reduction=\"mean\").to(device),\n",
        "        \"masked_topup_suscept_mse\": mrinr.nn.loss.WeightedMaskLoss(\n",
        "            \"mse\", reduction=\"mean\"\n",
        "        ).to(device),\n",
        "        \"masked_topup_corrected_b0_mse\": mrinr.nn.loss.WeightedMaskLoss(\n",
        "            \"mse\", reduction=\"mean\"\n",
        "        ).to(device),\n",
        "        \"det_J_def_neg_frac\": lambda J_deform: (\n",
        "            (torch.linalg.det(J_deform) < 0.0).sum()\n",
        "        )\n",
        "        / J_deform[..., 0, 0].numel(),\n",
        "    }\n",
        "\n",
        "    ### Training loop\n",
        "    if params.pretrain_to_atlas:\n",
        "        pretrain_optim = torch.optim.AdamW(\n",
        "            epinr.parameters(),\n",
        "            lr=params.optim_lr,\n",
        "            betas=params.optim_betas,\n",
        "            weight_decay=params.optim_weight_decay,\n",
        "            fused=True,\n",
        "        )\n",
        "        pretrain_results = pretrain_warped_atlas(\n",
        "            epinr=epinr,\n",
        "            max_iters=params.pretrain_epochs,\n",
        "            subj_data=subj_data,\n",
        "            params=params,\n",
        "            optim=pretrain_optim,\n",
        "            batch_size=None,\n",
        "        )\n",
        "        epinr = pretrain_results[\"epinr\"]\n",
        "        pretrain_loss = pretrain_results[\"train_losses\"]\n",
        "        del pretrain_optim\n",
        "\n",
        "    train_loss = {\n",
        "        \"epoch\": list(),\n",
        "        \"step\": list(),\n",
        "        \"loss_term\": list(),\n",
        "        \"val\": list(),\n",
        "        \"weighted\": list(),\n",
        "    }\n",
        "    accumulate_train_loss = collections.defaultdict(list)\n",
        "    val_scores = {\"epoch\": list(), \"step\": list(), \"metric\": list(), \"val\": list()}\n",
        "    step = 1\n",
        "    batch_step = 1\n",
        "    for epoch in range(1, params.epochs + 1):\n",
        "        optim.zero_grad()\n",
        "        epinr.train()\n",
        "        for i_epoch_batch, batch_dict in enumerate(coord_patch_dataloader):\n",
        "            if ((i_epoch_batch + 1) % grad_accumulate_steps != 0) and (\n",
        "                i_epoch_batch < (n_batches_per_epoch - 1)\n",
        "            ):\n",
        "                is_accumulating = True\n",
        "            else:\n",
        "                is_accumulating = False\n",
        "\n",
        "            coords = mrinr.nn.channels_as_coords(\n",
        "                batch_dict[\"coords\"].to(device), has_batch_dim=True\n",
        "            )\n",
        "\n",
        "            # Forward pass\n",
        "            pred_displacement_field_mm = epinr(\n",
        "                x=coords,\n",
        "                grid_fov=subj_data.b0_fov,\n",
        "                grid_min_coord=subj_data.b0_min_coord,\n",
        "            )\n",
        "            pred_deformation_field_mm = coords + pred_displacement_field_mm\n",
        "\n",
        "            # Sample the fixed t1w image.\n",
        "            fixed_t1w_sample = sample_t1w(epinr.apply_fixed_rigid_affine(coords))\n",
        "\n",
        "            # Sample moving image with Jacobian modulation at the predicted deformed\n",
        "            # coordinates.\n",
        "            J_pred = jacobian_matrix_batched(\n",
        "                pred_displacement_net=epinr,\n",
        "                x_coords=coords,\n",
        "                grid_fov=subj_data.b0_fov,\n",
        "                grid_min_coord=subj_data.b0_min_coord,\n",
        "                jac_of_warp_type=\"displacement\",\n",
        "                compile_jacobian=True,\n",
        "            )\n",
        "            if params.enable_jac_mod_in_training:\n",
        "                det_j_continuous_displace = 1 + J_pred[:, None, ..., 1, 1]\n",
        "                moving_b0_sample = det_j_continuous_displace * sample_b0(\n",
        "                    pred_deformation_field_mm\n",
        "                )\n",
        "            else:\n",
        "                moving_b0_sample = sample_b0(pred_deformation_field_mm)\n",
        "            b0_scanner_space_weight_mask = torch.maximum(\n",
        "                sample_b0_weight_mask(coords),\n",
        "                sample_t1w_weight_mask(epinr.apply_fixed_rigid_affine(coords)),\n",
        "            )\n",
        "            # Compute loss.\n",
        "            loss, loss_term_vals = loss_fn(\n",
        "                weighted_mse_sim=(\n",
        "                    moving_b0_sample,\n",
        "                    fixed_t1w_sample,\n",
        "                    b0_scanner_space_weight_mask,\n",
        "                ),\n",
        "                weighted_mi_sim=(\n",
        "                    moving_b0_sample,\n",
        "                    fixed_t1w_sample,\n",
        "                    b0_scanner_space_weight_mask,\n",
        "                ),\n",
        "                weighted_ncc_sim=(moving_b0_sample, fixed_t1w_sample),\n",
        "                weighted_lncc_sim=(moving_b0_sample, fixed_t1w_sample),\n",
        "                laplace_ncc_sim=(moving_b0_sample, fixed_t1w_sample),\n",
        "                laplace_mse_sim=(\n",
        "                    moving_b0_sample,\n",
        "                    fixed_t1w_sample,\n",
        "                    b0_scanner_space_weight_mask,\n",
        "                ),\n",
        "                jac_fro_regularization=J_pred,\n",
        "                bending_energy_regularization=(epinr, coords),\n",
        "                return_loss_terms=True,\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Record unweighted loss values, along with weighted sum.\n",
        "            for loss_term, loss_val in (\n",
        "                loss_term_vals\n",
        "                | {\"weighted_sum\": {\"value\": loss.detach().cpu().item(), \"weight\": 1.0}}\n",
        "            ).items():\n",
        "                accumulate_train_loss[loss_term].append(loss_val)\n",
        "\n",
        "            if not is_accumulating:\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                # Store averages of each loss term for this step.\n",
        "                for loss_term, acc_loss_vals_weights in accumulate_train_loss.items():\n",
        "                    l_vals = [l[\"value\"] for l in acc_loss_vals_weights]\n",
        "                    l_weight = [l[\"weight\"] for l in acc_loss_vals_weights]\n",
        "                    loss_value = np.mean(l_vals)\n",
        "                    loss_weight = np.median(l_weight)\n",
        "                    for l_val, is_weighted in zip(\n",
        "                        (loss_value, loss_value * loss_weight), (False, True)\n",
        "                    ):\n",
        "                        train_loss[\"epoch\"].append(epoch)\n",
        "                        train_loss[\"step\"].append(step)\n",
        "                        train_loss[\"loss_term\"].append(loss_term)\n",
        "                        train_loss[\"val\"].append(l_val)\n",
        "                        train_loss[\"weighted\"].append(is_weighted)\n",
        "                        if loss_term == \"weighted_sum\" and not is_weighted:\n",
        "                            loss_str = f\"{loss_value: 10.9f}\"\n",
        "                            pr(f\"{loss_str} | \", end=\"\")\n",
        "                accumulate_train_loss.clear()\n",
        "                # Only update step after the model was updated.\n",
        "                step += 1\n",
        "            # Always update batch_step, even if accumulating.\n",
        "            batch_step += 1\n",
        "        pr()\n",
        "        pr(f\" Epoch {epoch} completed.\")\n",
        "        # Epoch completed, update LR scheduler.\n",
        "        lr_scheduler.step()\n",
        "        if epoch == stop_rigid_aff_epoch:\n",
        "            pr(\"Stopping fixed rigid affine refinement.\")\n",
        "        if epoch >= stop_rigid_aff_epoch:\n",
        "            optim.param_groups[1][\"lr\"] = 0.0\n",
        "        pr(\"Learning rate(s):\")\n",
        "        pr([pg[\"lr\"] for pg in optim.param_groups])\n",
        "\n",
        "        # Validation step\n",
        "        epinr.eval()\n",
        "        with torch.no_grad():\n",
        "            val_results = validation_step(\n",
        "                epinr=epinr,\n",
        "                subj_data=subj_data,\n",
        "                params=params,\n",
        "                epoch=epoch,\n",
        "                step=step,\n",
        "                val_score_fns=val_score_fns,\n",
        "                b0_spacing=b0_spacing,\n",
        "                viz_validation_every=viz_validation_every\n",
        "                if not FULL_EXPERIMENT\n",
        "                else params.epochs,\n",
        "                pr=pr,\n",
        "            )\n",
        "            val_pred_displacement_fields_list.append(\n",
        "                val_results[\"pred_displacement_field_mm\"]\n",
        "            )\n",
        "            val_pred_undistorted_b0s_list.append(val_results[\"pred_undistorted_b0\"])\n",
        "            for k, v in val_results[\"val_scores\"].items():\n",
        "                val_scores[\"epoch\"].append(epoch)\n",
        "                val_scores[\"step\"].append(step)\n",
        "                val_scores[\"metric\"].append(k)\n",
        "                val_scores[\"val\"].append(v)\n",
        "        pr(\"\\n\")\n",
        "\n",
        "    final_pred_displacement_field_mm = val_pred_displacement_fields_list[-1]\n",
        "    train_loss = pd.DataFrame.from_dict(train_loss)\n",
        "    val_scores = pd.DataFrame.from_dict(val_scores)\n",
        "    pr(\"Saving training and validation scores...\")\n",
        "    train_loss.to_csv(train_scores_f, index=False)\n",
        "    val_scores.to_csv(val_scores_f, index=False)\n",
        "    # Save validation predicted displacement fields and undistorted b0s.\n",
        "    if not FULL_EXPERIMENT:\n",
        "        final_val_fig_f.unlink(missing_ok=True)\n",
        "        val_pred_displacement_fields_f.unlink(missing_ok=True)\n",
        "        val_undistorted_b0s_f.unlink(missing_ok=True)\n",
        "    val_pred_displacement_fields_arr = np.stack(\n",
        "        val_pred_displacement_fields_list, axis=-1\n",
        "    )\n",
        "    val_undistorted_b0s_arr = np.stack(val_pred_undistorted_b0s_list, axis=-1)\n",
        "    pr(\"Saving validation predicted displacement fields and undistorted b0s...\")\n",
        "    nib.save(\n",
        "        nib.Nifti1Image(\n",
        "            val_pred_displacement_fields_arr,\n",
        "            affine=subj_data.b0_affine.cpu().numpy().squeeze(0),\n",
        "        ),\n",
        "        val_pred_displacement_fields_f,\n",
        "    )\n",
        "    nib.save(\n",
        "        nib.Nifti1Image(\n",
        "            val_undistorted_b0s_arr,\n",
        "            affine=subj_data.b0_affine.cpu().numpy().squeeze(0),\n",
        "        ),\n",
        "        val_undistorted_b0s_f,\n",
        "    )\n",
        "    plt.savefig(final_val_fig_f)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training and validation scores.\n",
        "    if not isinstance(train_loss, pd.DataFrame):\n",
        "        train_loss = pd.DataFrame.from_dict(train_loss)\n",
        "        val_scores = pd.DataFrame.from_dict(val_scores)\n",
        "\n",
        "    sns.relplot(\n",
        "        train_loss,\n",
        "        x=\"step\",\n",
        "        y=\"val\",\n",
        "        row=\"weighted\",\n",
        "        col=\"loss_term\",\n",
        "        hue=\"loss_term\",\n",
        "        kind=\"line\",\n",
        "        facet_kws={\"sharey\": \"row\", \"sharex\": True},\n",
        "    )\n",
        "    plt.title(\"Training Losses over Steps\")\n",
        "    plt.savefig(train_loss_plot_f)\n",
        "    if not FULL_EXPERIMENT:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.clf()\n",
        "    sns.relplot(\n",
        "        val_scores,\n",
        "        x=\"step\",\n",
        "        y=\"val\",\n",
        "        col=\"metric\",\n",
        "        hue=\"metric\",\n",
        "        kind=\"line\",\n",
        "        facet_kws={\"sharey\": False, \"sharex\": True},\n",
        "    )\n",
        "    plt.savefig(val_scores_plot_f)\n",
        "    if not FULL_EXPERIMENT:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.clf()\n",
        "\n",
        "    if FULL_EXPERIMENT:\n",
        "        # Save out final predicted displacement field.\n",
        "        final_pred_displacement_mm_f = (\n",
        "            subj_result_dir\n",
        "            / f\"epinr_final_pred_displace_field_dir-{subj_data.pe_dir}_mm.nii.gz\"\n",
        "        )\n",
        "        nib.save(\n",
        "            nib.Nifti1Image(\n",
        "                final_pred_displacement_field_mm,\n",
        "                affine=subj_data.b0_affine.cpu().numpy().squeeze(0),\n",
        "            ),\n",
        "            final_pred_displacement_mm_f,\n",
        "        )\n",
        "\n",
        "        final_pred_suscept_field_hz = (\n",
        "            (final_pred_displacement_field_mm * (-1 if subj_data.pe_dir == \"ap\" else 1))\n",
        "            / b0_spacing[0, 1].cpu().numpy()\n",
        "        ) / subj_data.total_readout_time_s\n",
        "        final_pred_displacement_hz_f = (\n",
        "            subj_result_dir / \"epinr_final_pred_suscept_field_hz.nii.gz\"\n",
        "        )\n",
        "        nib.save(\n",
        "            nib.Nifti1Image(\n",
        "                final_pred_suscept_field_hz,\n",
        "                affine=subj_data.b0_affine.cpu().numpy().squeeze(0),\n",
        "            ),\n",
        "            final_pred_displacement_hz_f,\n",
        "        )\n",
        "        torch.save(epinr.state_dict(), subj_result_dir / \"epinr_final_model.pt\")\n",
        "        pr(f\"Finished subject {subj_data.dataset_name}, {subj_data.subj_id}.\")\n",
        "\n",
        "if FULL_EXPERIMENT:\n",
        "    # Move the tmp result dir to the final result dir.\n",
        "    print(\n",
        "        f\"Moving temporary result dir\\n{tmp_result_dir}\\nto final directory\\n{final_result_dir}\"\n",
        "    )\n",
        "    tmp_result_dir.rename(final_result_dir)\n",
        "print(\"Done\")\n",
        "# -------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26e373d4",
      "metadata": {},
      "source": [
        "### Reconstructed Displacement Field Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d7639c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import skimage\n",
        "import skimage.filters\n",
        "\n",
        "b01 = subj_data.b0.detach().squeeze(0).squeeze(0).cpu().numpy()\n",
        "b02 = moving_b0_sample.detach().squeeze(0).squeeze(0).cpu().numpy()\n",
        "b03 = subj_data.topup_corrected_b0.detach().squeeze(0).squeeze(0).cpu().numpy()\n",
        "t1w1 = fixed_t1w_sample.detach().squeeze(0).squeeze(0).cpu().numpy()\n",
        "\n",
        "s = 0.5\n",
        "dog_b0 = skimage.filters.difference_of_gaussians(\n",
        "    b01,\n",
        "    low_sigma=s,\n",
        ")\n",
        "dog_epinr_b0 = skimage.filters.difference_of_gaussians(\n",
        "    b02,\n",
        "    low_sigma=s,\n",
        ")\n",
        "# my_dog_epinr_b0 = laplace(moving_b0_sample)\n",
        "dog_topup_b0 = skimage.filters.difference_of_gaussians(\n",
        "    b03,\n",
        "    low_sigma=s,\n",
        ")\n",
        "dog_t1w = skimage.filters.difference_of_gaussians(\n",
        "    # 1 - t1w1,\n",
        "    t1w1,\n",
        "    low_sigma=s,\n",
        ")\n",
        "# k = 9\n",
        "# l_b0 = skimage.filters.laplace(b01, ksize=k)\n",
        "# l_b0 = l_b0 / np.abs(l_b0).sum((-1, -2), keepdims=True)\n",
        "# l_topup_b0 = skimage.filters.laplace(b03, ksize=k)\n",
        "# l_topup_b0 = l_topup_b0 / np.abs(l_topup_b0).sum((-1, -2), keepdims=True)\n",
        "# l_epinr_b0 = skimage.filters.laplace(b02, ksize=k)\n",
        "# l_t1w = skimage.filters.laplace(t1w1, ksize=k)\n",
        "# l_t1w = l_t1w / np.abs(l_t1w).sum((-1, -2), keepdims=True)\n",
        "plt.figure(dpi=150)\n",
        "mrinr.viz.plot_1_ch_multi_slice_compare_vols(\n",
        "    dog_b0,\n",
        "    dog_epinr_b0,\n",
        "    dog_topup_b0,\n",
        "    dog_t1w,\n",
        "    colorbars=\"rows\",\n",
        "    axial_slice_idx=(0.35, 0.5),\n",
        "    saggital_slice_idx=0.45,\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "206c5023",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mrinr2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
